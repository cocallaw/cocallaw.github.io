[ { "title": "Proactively Monitor Azure Service Retirements with AzRetirementMonitor", "url": "/posts/Proactively-monitor-Azure-retirements-AzRetirementMonitor/", "categories": "Azure, PowerShell", "tags": "Azure, PowerShell, Automation, DevOps, monitoring, azure-advisor", "date": "2026-01-02 12:00:00 -0500", "content": "Azureâ€™s cloud platform evolves rapidly, bringing new capabilities and improvements regularly. However, this evolution also means that older services, features, and APIs are periodically retired or deprecated. Missing these retirement notifications can lead to unexpected service disruptions, security vulnerabilities, compliance issues, and costly emergency migrations. Azure provides several built-in tools to help you stay informed about retirements, including the Azure Advisor Retirements Workbook, Service Health alerts, and notifications within the Azure portal. These are excellent resources that many organizations rely on. However, different teams often need different approaches based on their workflows, automation requirements, and operational preferences. To provide an additional option for retirement monitoring, I built AzRetirementMonitor, a PowerShell module that helps you proactively identify Azure resources affected by upcoming retirements by querying the Azure Advisor API across all your subscriptions. This tool complements existing Azure monitoring capabilities by offering a PowerShell-native approach that can be easily integrated into scripts, automation workflows, and CI/CD pipelines. The Problem: Why Service Retirement Monitoring Matters As organizations scale their Azure presence, keeping track of retirement notices becomes increasingly difficult. Consider these scenarios: A critical VM size youâ€™re using is being retired in 90 days An API version your application depends on will stop working next month A storage service tier is being deprecated, requiring data migration A security feature is being replaced with a different implementation While Azure provides several ways to monitor retirements, not every team or workflow fits neatly into those tools. Teams working outside of the Azure portal with PowerShell or other tool sets may benefit from having retirement data available in their existing automation workflows. AzRetirementMonitor provides this capability by offering automated, centralized visibility into all retirement recommendations across your Azure estate through familiar PowerShell commands. What is AzRetirementMonitor? AzRetirementMonitor is an open-source PowerShell module that leverages the Azure Advisor API to retrieve service upgrade and retirement recommendations. It specifically focuses on HighAvailability category recommendations with the ServiceUpgradeAndRetirement subcategory, giving you a filtered view of exactly what matters for service continuity. This module complements Azureâ€™s native monitoring capabilities by providing a PowerShell approach that integrates naturally into script-based workflows, making it easy to incorporate retirement checks into existing workflows and management tasks. Key Features Multi-subscription support: Scan all your Azure subscriptions in one command Flexible authentication: Works with both Azure CLI and Az PowerShell module Multiple export formats: Generate reports in CSV, JSON, or HTML Detailed recommendations: Get actionable solutions with links to documentation PowerShell 7+ compatible: Modern PowerShell for cross-platform support Getting Started Prerequisites You will need: PowerShell 7.0 or later Either Azure CLI or the Az.Accounts PowerShell module for authentication Appropriate permissions to read Azure Advisor recommendations Installation The easiest way to install AzRetirementMonitor is from the PowerShell Gallery: Install-Module -Name AzRetirementMonitor -Scope CurrentUser Alternatively, you can clone the repository from GitHub: git clone https://github.com/cocallaw/AzRetirementMonitor.git Import-Module ./AzRetirementMonitor/AzRetirementMonitor.psd1 Authentication AzRetirementMonitor supports two authentication methods: Option 1: Azure CLI (Default and Recommended) # Login to Azure az login # Connect the module Connect-AzRetirementMonitor Option 2: Az PowerShell Module # Install Az.Accounts if needed Install-Module -Name Az.Accounts -Scope CurrentUser # Connect to Azure Connect-AzAccount # Connect the module using Az PowerShell Connect-AzRetirementMonitor -UseAzPowerShell Using AzRetirementMonitor Basic Usage Once authenticated, getting retirement recommendations is straightforward: # Get all retirement recommendations across all subscriptions Get-AzRetirementRecommendation The output includes: Subscription ID and resource ID Resource name and category Impact level (High, Medium, Low) Problem description Recommended solution Links to documentation Last updated timestamp Targeting Specific Subscriptions If you want to focus on particular subscriptions: Get-AzRetirementRecommendation -SubscriptionId \"sub-id-1\", \"sub-id-2\" Generating Reports Export recommendations to share with your team: # CSV for spreadsheet analysis Get-AzRetirementRecommendation | Export-AzRetirementReport -OutputPath \"report.csv\" -Format CSV # JSON for programmatic processing Get-AzRetirementRecommendation | Export-AzRetirementReport -OutputPath \"report.json\" -Format JSON # HTML for easy viewing and sharing Get-AzRetirementRecommendation | Export-AzRetirementReport -OutputPath \"report.html\" -Format HTML The HTML report provides a clean, formatted view of all retirement recommendations thatâ€™s easy to share with stakeholders: Example of a generated HTML retirement report Understanding Retirement Metadata To get information about retirement recommendation types: Get-AzRetirementMetadataItem This provides metadata about the different types of retirement recommendations Azure Advisor can generate. Complete Workflow Example Hereâ€™s how I recommend using AzRetirementMonitor in your operations: # 1. Authenticate Connect-AzRetirementMonitor # 2. Retrieve all retirement recommendations $recommendations = Get-AzRetirementRecommendation # 3. Review critical items $recommendations | Where-Object { $_.Impact -eq 'High' } | Format-Table ResourceName, Problem, Solution # 4. Generate an HTML report for stakeholder review $recommendations | Export-AzRetirementReport -OutputPath \"weekly-retirement-report.html\" -Format HTML # 5. Export CSV for tracking in your change management system $recommendations | Export-AzRetirementReport -OutputPath \"retirement-tracking.csv\" -Format CSV Example Output Hereâ€™s what a typical recommendation looks like: SubscriptionId : 12345678-1234-1234-1234-123456789012 ResourceId : /subscriptions/.../resourceGroups/myRG/providers/Microsoft.Compute/virtualMachines/myVM ResourceName : myVM Category : HighAvailability Impact : High Problem : Virtual machine is using a retiring VM size Solution : Migrate to a supported VM size before the retirement date Description : Basic A-series VM sizes will be retired on August 31, 2024 LastUpdated : 2024-01-15T10:30:00Z IsRetirement : True LearnMoreLink : https://learn.microsoft.com/azure/virtual-machines/sizes-previous-gen Contributing AzRetirementMonitor is open source and welcomes contributions! Whether youâ€™re reporting bugs, requesting features, or submitting pull requests, your input helps make the module better for everyone. The project follows standard PowerShell module conventions and includes Pester tests. Check out the contributing guidelines in the repository for details. Resources GitHub Repository: https://github.com/cocallaw/AzRetirementMonitor PowerShell Gallery: https://www.powershellgallery.com/packages/AzRetirementMonitor Azure Advisor Documentation: https://learn.microsoft.com/azure/advisor/ Final Thoughts Proactive monitoring of Azure service retirements isnâ€™t just about avoiding downtime; itâ€™s about maintaining and improving resiliency while giving your team the time needed for proper planning, testing, and migration. AzRetirementMonitor makes this monitoring accessible through simple PowerShell commands that can be integrated into your existing workflows. Whether youâ€™re managing a handful of resources or a massive Azure estate, staying ahead of service retirements is crucial. Give AzRetirementMonitor a try and let me know what you think! Have questions or suggestions? Feel free to open an issue on GitHub or reach out. Iâ€™m always looking to improve the module based on real-world usage. Happy monitoring! ðŸš€" }, { "title": "Automating Let's Encrypt SSL Certificates on Synology DSM with acme.sh and Cloudflare DNS", "url": "/posts/Automating-Lets-Encrypt-SSL-Certificates-on-Synology/", "categories": "Tutorial, Synology", "tags": "Synology, lets-encrypt, SSL, acme.sh, Cloudflare, dns-challenge, Automation", "date": "2025-12-14 12:00:00 -0500", "content": "Managing SSL certificates can be tedious, especially when dealing with resources that arenâ€™t publicly accessible. Synology currently provides the ability to automate certificate management through its interface, but it has limitations. If you want to use a certificate from Letâ€™s Encrypt, you have to rely on HTTP challenges, which require your Synology to be accessible over the internet. This can be a security concern and isnâ€™t always feasible for home labs or internal services. However, by SSHing into your Synology and using the acme.sh script with Cloudflare DNS validation, you can fully automate the issuance and renewal of Letâ€™s Encrypt certificates without exposing your Synology to the internet. In this guide, Iâ€™ll walk you through setting up the certificate generation and automation using acme.sh and Cloudflareâ€™s to perform DNS-01 challenges for your domain. Why DNS Challenge? The DNS-01 challenge method offers several advantages over traditional HTTP-01 challenges: No port forwarding required - Your Synology doesnâ€™t need to be publicly accessible Wildcard certificate support - Secure multiple subdomains with a single certificate Works behind firewalls - Perfect for internal or VPN-only accessible systems Cleaner and more secure - No temporary files exposed on your web server Prerequisites Before starting, ensure you have: A Synology NAS running DSM 7.x SSH access enabled (Control Panel â†’ Terminal &amp; SNMP â†’ Enable SSH service) A domain managed by Cloudflare (free tier works) Cloudflare API credentials Basic command line familiarity Step 1: Create a Dedicated Certificate Management User For better security and organization, create a dedicated user for managing certificates. This user will own the acme.sh installation and handle certificate renewals. Creating the User in DSM Log into your Synology DSM web interface as an administrator Navigate to Control Panel â†’ User &amp; Group Click Create to add a new user Configure the user with these settings: Name: certadmin Description: Certificate Management Account Email: (optional) Password: Set a strong password (youâ€™ll need this for deployment) Disallow user to change password: (your preference) On the Permissions tab, you can leave most services unchecked On the Applications tab, no special permissions are needed Click Apply to create the user Grant Administrator Privileges The certadmin user needs administrator privileges to deploy certificates to DSM: Still in Control Panel â†’ User &amp; Group Select the Group tab Click on the administrators group Click Edit Check the box next to certadmin to add them to the administrators group Click OK to save Enable SSH Access Now SSH into your Synology as an administrator user to prepare the environment: ssh certadmin@your-synology-ip The certadmin user is now ready to manage certificates. Step 2: Obtain Cloudflare API Credentials Youâ€™ll need Cloudflare API credentials to allow acme.sh to create DNS validation records. Log into your Cloudflare dashboard Navigate to My Profile â†’ API Tokens Create a token with Zone:DNS:Edit permissions for your domain Save the token securely - youâ€™ll need it shortly Alternatively, you can use your Global API Key (less secure): Find it under My Profile â†’ API Tokens â†’ Global API Key Step 3: Download and Install acme.sh Download the latest acme.sh release from GitHub: wget -O /tmp/acme.sh.zip https://github.com/acmesh-official/acme.sh/archive/master.zip Extract it to a permanent location: sudo 7z x -o/usr/local/share /tmp/acme.sh.zip Rename the extracted directory: sudo mv /usr/local/share/acme.sh-master/ /usr/local/share/acme.sh Set appropriate ownership (if using the certadmin user): sudo chown -R certadmin /usr/local/share/acme.sh/ Step 4: Configure Cloudflare DNS API Set your Cloudflare credentials as environment variables. For API Token (recommended): export CF_Token=\"your_cloudflare_api_token_here\" export CF_Account_ID=\"your_cloudflare_account_id\" export CF_Zone_ID=\"your_zone_id\" Or for Global API Key: export CF_Key=\"your_cloudflare_global_api_key\" export CF_Email=\"your_cloudflare_email@example.com\" These credentials will be saved by acme.sh for future renewals. Step 5: Issue Your Certificate Navigate to the acme.sh directory: cd /usr/local/share/acme.sh Request a certificate using the DNS challenge method: ./acme.sh --server letsencrypt --issue -d \"exampledomain.com\" --dns dns_cf --home $PWD For a wildcard certificate, add the wildcard domain: ./acme.sh --server letsencrypt --issue -d \"exampledomain.com\" -d \"*.exampledomain.com\" --dns dns_cf --home $PWD The script will: Generate a certificate signing request Create a TXT record in your Cloudflare DNS Wait for DNS propagation Complete the ACME challenge Retrieve your certificate Step 6: Deploy to Synology DSM Once the certificate is issued, deploy it to your Synology system: ./acme.sh -d \"exampledomain.com\" --deploy --deploy-hook synology_dsm --home $PWD Before running this command, you may need to set Synology-specific environment variables: export SYNO_Username=\"your_admin_username\" export SYNO_Password=\"your_admin_password\" export SYNO_Certificate=\"your_certificate_description\" export SYNO_Create=1 The deploy hook will: Connect to the Synology API Upload the certificate and private key Create or update the certificate in DSM Optionally restart services using the certificate Step 7: Verify Installation Log into your Synology DSM web interface: Go to Control Panel â†’ Security â†’ Certificate You should see your new Letâ€™s Encrypt certificate listed Click Configure to assign it to specific services Select which services should use this certificate Step 8: Configure Automatic Renewal with Synology Task Scheduler While acme.sh can install its own cron job, using Synologyâ€™s Task Scheduler provides better integration and easier management through the DSM interface. Create a Scheduled Task Log into DSM and navigate to Control Panel â†’ Task Scheduler Click Create â†’ Scheduled Task â†’ User-defined script Configure the General settings: Task: Renew Let's Encrypt Certificate User: Select certadmin (this is critical - it must run as the certadmin user) Enabled: Check this box Configure the Schedule tab: Run on the following days: Select Daily First run time: Set to a time when your NAS is reliably on (e.g., 3:00 AM) Frequency: Select Once Last run time: Leave blank Configure the Task Settings tab: Send run details by email: (optional, recommended for monitoring) User-defined script: Enter the renewal command: /usr/local/share/acme.sh/acme.sh --renew -d \"*.exampledomain.com\" --home /usr/local/share/acme.sh --server letsencrypt For a non-wildcard certificate, use: /usr/local/share/acme.sh/acme.sh --renew -d \"exampledomain.com\" --home /usr/local/share/acme.sh --server letsencrypt Click OK to save the task Testing the Scheduled Task Before waiting for the scheduled run, test the task manually: In Task Scheduler, select your renewal task Click Run at the top Check the results by clicking Action â†’ View Result The task will check if renewal is needed (certificates are renewed when theyâ€™re within 60 days of expiration). During the initial test, you should see output indicating the certificate is still valid and renewal isnâ€™t needed yet. Force a Renewal Test To verify everything works correctly, you can force a renewal via SSH: sudo -u certadmin /usr/local/share/acme.sh/acme.sh --renew -d \"*.exampledomain.com\" --home /usr/local/share/acme.sh --server letsencrypt --force This will renew the certificate immediately, allowing you to verify the entire process works before the certificate actually needs renewal. Monitoring Renewals Enable email notifications in the Task Scheduler to receive alerts about renewal status: Edit your scheduled task Check Send run details by email Send run details only when the script terminates abnormally: (your preference) This ensures youâ€™re notified if renewals fail for any reason. Troubleshooting DNS Propagation Issues If the challenge fails due to DNS propagation, you can increase the wait time: ./acme.sh --issue -d \"exampledomain.com\" --dns dns_cf --dnssleep 120 --home $PWD Permission Errors If you encounter permission issues, ensure the acme.sh directory has correct ownership: sudo chown -R $(whoami) /usr/local/share/acme.sh/ Certificate Not Appearing in DSM Verify your Synology credentials are correct and the certadmin user has admin privileges (is in the administrators group). You can also manually import the certificate through the DSM interface: Certificate: /usr/local/share/acme.sh/exampledomain.com/exampledomain.com.cer Private Key: /usr/local/share/acme.sh/exampledomain.com/exampledomain.com.key Intermediate Certificate: /usr/local/share/acme.sh/exampledomain.com/ca.cer Scheduled Task Not Running If your Task Scheduler task fails, check these common issues: Verify the task is running as the certadmin user (not root or another user) Ensure certadmin has ownership of /usr/local/share/acme.sh/ Check the task result logs in Task Scheduler for specific error messages Verify the domain name in your renewal command matches exactly what you issued Checking Certificate Status View information about your certificates: ./acme.sh --list --home $PWD View detailed information for a specific certificate: ./acme.sh --info -d \"exampledomain.com\" --home $PWD Security Considerations Protect your credentials: The acme.sh configuration stores your Cloudflare API credentials. Ensure proper file permissions. Use API Tokens over Global API Keys: Tokens can be scoped to specific permissions and revoked independently. Limit SSH access: Consider disabling SSH when not needed or restricting it to specific IP addresses. Regular backups: Include your acme.sh directory in your backup routine to preserve certificate history and configurations. Rotating Cloudflare API Credentials Cloudflare API tokens have expiration dates and should be rotated periodically for security. When your token expires or you need to rotate credentials, follow these steps: Generate a New API Token Log into your Cloudflare dashboard Navigate to My Profile â†’ API Tokens Create a new token with the same permissions (or click Edit on your existing token to extend/regenerate it) Copy the new token immediately - Cloudflare only shows it once Update acme.sh Configuration The API credentials are stored in acme.shâ€™s configuration file. To update them: # SSH into your Synology as certadmin or use sudo ssh certadmin@your-synology-ip Edit the account configuration file: cd /usr/local/share/acme.sh nano account.conf Look for these lines and update with your new credentials: For API Token: CF_Token='your_new_cloudflare_api_token_here' CF_Account_ID='your_cloudflare_account_id' CF_Zone_ID='your_zone_id' Or for Global API Key: CF_Key='your_new_cloudflare_global_api_key' CF_Email='your_cloudflare_email@example.com' Save the file (Ctrl+O, Enter, Ctrl+X in nano). Verify the New Credentials Test that the new credentials work by forcing a renewal: ./acme.sh --renew -d \"exampledomain.com\" --home /usr/local/share/acme.sh --server letsencrypt --force If successful, your new API credentials are working correctly. The scheduled task will now use these updated credentials automatically. Set a Reminder Since API tokens expire, set a calendar reminder to rotate your token before expiration: Check your tokenâ€™s expiration date in the Cloudflare dashboard Set a reminder 1-2 weeks before expiration Follow this rotation process when the reminder triggers Updating acme.sh Keep acme.sh updated to benefit from bug fixes and new features: cd /usr/local/share/acme.sh ./acme.sh --upgrade --home $PWD To enable automatic updates: ./acme.sh --upgrade --auto-upgrade --home $PWD Conclusion With acme.sh and Cloudflare DNS validation, you now have a fully automated SSL certificate solution for your Synology NAS. Certificates will renew automatically, and you donâ€™t need to expose your system to the internet. This setup is particularly valuable for home labs, internal services, or any Synology deployment that prioritizes security and convenience. The combination of Letâ€™s Encryptâ€™s free certificates, acme.shâ€™s automation capabilities, and Cloudflareâ€™s DNS API creates a robust, maintenance-free certificate management system that just works. Additional Resources acme.sh GitHub Repository acme.sh DNS API Wiki Letâ€™s Encrypt Documentation Cloudflare API Documentation" }, { "title": "Deploying AVD and retrieving the Registration Token with Bicep", "url": "/posts/Deploying-AVD-with-Bicep/", "categories": "Azure, AVD, Bicep", "tags": "AVD, Bicep, Azure Virtual Desktop, Registration Token, Infrastructure as Code", "date": "2025-06-16 13:00:00 -0400", "content": "Overview Azure Virtual Desktop (AVD) is a robust solution for delivering virtualized applications and desktops to users. To simplify the deployment and management of AVD environments, administrators often rely on Infrastructure as Code (IaC) tools like Azure Bicep. One of the more nuanced challenges when deploying AVD with Bicep is handling the registration token required by session hosts. This token is typically short-lived, cannot be retrieved through a GET operation due to recent API changes, and must be passed securely into session host configuration steps such as DSC extensions. Previously, certain API versions returned the registration token for a host pool on a GET (read) operation. While convenient, this raised a security concern: any user with Reader access could view an active token. To address this, Microsoft updated the AVD API in version 2023-09-05 so the registration token is only returned during a PUT or POST (write/update) operation. This change prevents passive access to registration tokens and aligns with the principle of least privilege. See Microsoftâ€™s announcement for more. Before the API change, it was easy to retrieve the registration token for a host pool by simply querying the host pool resource, and many deployment tools relied on this functionality. Now with the API updated, how can we still easily deploy a new Host Pool resource and retrieve the registration token? In this post, we will discuss the use of two Bicep templates: main.bicep for deploying the AVD resources and token.bicep for generating and retrieving the registration token securely. Key Features of the Templates You can find the full Bicep templates, parameter files, and example usage in the public GitHub repository: ðŸ‘‰ Az-AVD-Hostpool-Bicep main.bicep The main.bicep template provisions the core resources required for an AVD environment, including: Host Pools: Manage user sessions efficiently. Session Hosts: Virtual machines that host user sessions. Application Groups: Logical grouping of applications for users. Workspaces: Centralized access points for published resources. token.bicep The token.bicep template securely generates and manages registration tokens. These tokens are essential for registering session hosts with the host pool. Why Use a Separate Token Template Recent changes in the AVD API have enhanced security by preventing registration tokens from being retrieved using standard read/get operations. This ensures that tokens are only available during their creation, reducing the risk of unauthorized access. By isolating token generation into a dedicated module (token.bicep), we can trigger the host pool update operation needed to create a new token without redeploying the entire environment. This separation also ensures better reusability and clarity in deployment flows. How the Token is Returned The token.bicep file takes in the necessary information about the Host Pool resources that was created earlier in the template through the use of parameters and has the Host Pool generate an updated token by using the registrationTokenOperation: 'Update' property. The registration token is retrieved from the response by using the listRegistrationTokens() function on the hostPoolTokenUpdate resource. main.bicep hostPoolRegistrationToken Module // Update the Host Pool to return Registration Token module hostPoolRegistrationToken 'token.bicep' = { name: 'hostPoolRegistrationToken' params: { hostPoolName: hostPoolName tags: hostPool.tags location: hostPool.location hostPoolType: hostPool.properties.hostPoolType friendlyName: hostPool.properties.friendlyName loadBalancerType: hostPool.properties.loadBalancerType preferredAppGroupType: hostPool.properties.preferredAppGroupType maxSessionLimit: hostPool.properties.maxSessionLimit startVMOnConnect: hostPool.properties.startVMOnConnect validationEnvironment: hostPool.properties.validationEnvironment agentUpdate: hostPool.properties.agentUpdate } dependsOn: [ desktopAppGroup workspace ] } token.bicep hostPoolTokenUpdate resource resource hostPoolTokenUpdate 'Microsoft.DesktopVirtualization/hostPools@2024-04-03' = { name: hostPoolName location: location tags: tags properties: { friendlyName: friendlyName hostPoolType: hostPoolType loadBalancerType: loadBalancerType preferredAppGroupType: preferredAppGroupType maxSessionLimit: maxSessionLimit startVMOnConnect: startVMOnConnect validationEnvironment: validationEnvironment agentUpdate: agentUpdate // Update the registration info with a new token registrationInfo: { expirationTime: dateTimeAdd(baseTime, tokenValidityLength) registrationTokenOperation: 'Update' } } } The token.bicep template outputs the registration token that is valid for the configured duration, defaulting to eight hours. This token must be used immediately after creation, as it cannot be retrieved later. @secure() output registrationToken string = first(hostPoolTokenUpdate.listRegistrationTokens().value)!.token Why This Approach is Necessary Restricting token retrieval aligns with best practices for secure token management. By ensuring that tokens are generated and consumed securely, this approach minimizes the risk of unauthorized access and enhances the overall security of the AVD environment. To enhance the security of the deployment, the registration token can be stored as a secret in Azure Key Vault or another secure storage solution. This allows for controlled access to the token by different solutions or scripts that need to register session hosts with the host pool without exposing it in plain text. Deployment Instructions 1. Get the Bicep Templates You can find the full Bicep templates, parameter files, and example usage in the public GitHub repository: Az-AVD-Hostpool-Bicep 2 Install the Azure CLI and Bicep CLI Ensure you have the Azure CLI and Bicep CLI installed on your machine. You can install them using the following commands: How to install the Azure CLI (Microsoft Learn) How to install the Bicep CLI (Miceosoft Learn) Once you have the Azure CLI and Bicep installed, sign in to your Azure account and set the appropriate subscription context: az login az account set --subscription &lt;your-subscription-id&gt; 2. Prepare Parameter File Update the main.bicepparam file to provide deployment parameters specific to your environment. using 'main.bicep' /*AVD Config*/ param hostPoolName = 'myAVDHostPool' param sessionHostCount = 2 param maxSessionLimit = 5 /*VM Config*/ param vmNamePrefix = 'myAVDVM' param adminUsername = 'superAdmin' param adminPassword = 'NotaPassword!' /*Network Config*/ param subnetName = 'mySubnet' param vnetName = 'myVNet' param vnetResourceGroup = 'myNetworkRG' 3. Deploy the Templates Run the following command to deploy the entire solution, including token retrieval: az deployment group create \\ --resource-group &lt;resource-group-name&gt; \\ --template-file main.bicep \\ --parameters main.bicepparam The main.bicep file internally calls the token.bicep module and injects the token into the session host configuration as part of the DSC extension setup. Conclusion These Bicep templates simplify and secure the deployment of Azure Virtual Desktop environments. By retrieving the token as part of the deployment you can simplify your deployment process and in some cases reduce the number of steps needed." }, { "title": "Identify AzureAD PowerShell Users in your Tenant before Retirement", "url": "/posts/Identify-Legacy-AAD-PS-Users/", "categories": "Azure, PowerShell, Entra", "tags": "AzureAD, Microsoft Graph, PowerShell, Entra, Reporting, Security", "date": "2025-05-27 13:00:00 -0400", "content": "Microsoft is retiring the legacy AzureAD and MSOnline PowerShell modules. These modules have been widely used for years to manage users, groups, roles, and other directory objects in Azure Active Directory (now Entra ID). With their deprecation, itâ€™s critical to identify any users or automation that still rely on them â€” and plan a migration. In this post, Iâ€™ll walk you through how to: Identify sign-ins using the AzureAD PowerShell App ID with Kusto Query Language (KQL) Use a PowerShell script to query sign-in logs and generate reports Export and report this data via PowerShell in CSV and HTML formats Whatâ€™s Being Retired? The following modules are being deprecated: AzureAD (AzureAD.Standard.Preview as well) MSOnline These modules authenticate using the app registration (1b730954-1685-4b74-9bfd-dac224a7b894) that shows up in sign-in logs. Depending on the use case, customers can use either Microsoft Graph PowerShell or Microsoft Entra PowerShell replacement. Option 1: Find AzureAD PowerShell Sign-In Activity with KQL You can use Kusto Query Language (KQL) to identify sign-ins tied to the legacy AzureAD app in your Entra ID tenant. This is useful for quickly finding users or scripts that still rely on the AzureAD PowerShell module. If you have your sign-in logs being sent to a Log Analytics Workspace or Microsoft Sentinel you can use this Kusto query as a starting point to filter sign-in logs for the AzureAD PowerShell app ID and return relevant details. KQL Query SigninLogs | where AppId == \"1b730954-1685-4b74-9bfd-dac224a7b894\" // AzureAD PowerShell app ID | project UserPrincipalName, IPAddress, ResourceDisplayName, AuthenticationRequirement, ConditionalAccessStatus, Status, CreatedDateTime | order by CreatedDateTime desc Option 2: Use a PowerShell Script to Query Logs and Generate Reports If you prefer a more automated approach, you can use a PowerShell script to query the sign-in logs for the AzureAD PowerShell app and generate reports in both CSV and HTML formats. This script will help you identify users that still rely on the legacy AzureAD module and easily filter. The full script is available on GitHub at cocallaw/Entra-AAD-PS-Retirement Prerequisites PowerShell Core Ensure you have PowerShell Core installed on your machine. This script is designed to run on PowerShell Core (7.x) for cross-platform compatibility. This script has been tested on Windows and MacOS only. Microsoft Graph PowerShell This script utilizes the Microsoft Graph PowerShell SDK in order to retrieve logs from the tenant. If you havenâ€™t done so already, install it using the following command: Install-Module Microsoft.Graph -Scope CurrentUser Permissions Ensure you have the following permissions granted to your account to run the script: AuditLog.Read.All Directory.Read.All IdentityRiskEvent.Read.All What the Script Does This script connects to Microsoft Graph, retrieves sign-in logs for the AzureAD PowerShell app, and correlates them with identity risk events. It then generates a report in both CSV and HTML formats for easy review. Sample Output CSV Report The script generates a CSV file with the following format: \"UserPrincipalName\",\"AppDisplayName\",\"IPAddress\",\"Location\",\"SignInTime\",\"Status\",\"RiskLevel\",\"CorrelationId\" \"jane.doe@contoso.com\",\"Azure Active Directory PowerShell\",\"2605:&lt;IPV6-Address&gt;:40bd\",\"Charlotte, North Carolina, US\",\"5/26/2025 8:46:59â€¯PM\",\"Success\",\"none\",\"0d3b4be7-ce67-4adf-a8c7-506078f9a013\" \"john.doe@contoso.com\",\"Azure Active Directory PowerShell\",\"XXX.YYY.ZZZ.VVV\",\"Charlotte, North Carolina, US\",\"5/26/2025 8:15:56â€¯PM\",\"Success\",\"none\",\"932246df-d54b-4b4d-94ec-d29427766b11\" \"john.doe@contoso.com\",\"Azure Active Directory PowerShell\",\"XXX.YYY.ZZZ.VVV\",\"Charlotte, North Carolina, US\",\"5/26/2025 8:15:24â€¯PM\",\"Success\",\"none\",\"a9f18460-db0a-4ab6-8d6b-cc0f8c468a5a\" \"john.doe@contoso.com\",\"Azure Active Directory PowerShell\",\"XXX.YYY.ZZZ.VVV\",\"Charlotte, North Carolina, US\",\"5/20/2025 5:34:50â€¯PM\",\"Success\",\"none\",\"279e3fe5-34ca-4bbf-a208-906d8f25c90c\" HTML Report The script generates an interactive HTML report with sortable columns, which can be opened in any web browser. The HTML report includes the same data as the CSV but is formatted for easier viewing and analysis. Key Components Prerequisites &amp; Authentication Validates Microsoft Graph PowerShell module is installed and loaded Connects to Microsoft Graph with required permissions Handles existing connections intelligently # Check if Microsoft Graph commands are available if (-not (Get-Command Get-MgContext -ErrorAction SilentlyContinue)) { # Only then check if module is installed if (-not (Get-Module -ListAvailable -Name Microsoft.Graph)) { $WarningPreference = 'Continue' Write-Warning \"Microsoft.Graph module is not installed. Please install it using: Install-Module Microsoft.Graph -Scope CurrentUser\" exit } # Try to load the module silently Import-Module Microsoft.Graph.Authentication -DisableNameChecking -Force -ErrorAction SilentlyContinue } # Connect with required permissions $requiredScopes = @(\"AuditLog.Read.All\", \"Directory.Read.All\", \"IdentityRiskEvent.Read.All\") if (-not (Get-MgContext)) { Connect-MgGraph -Scopes $requiredScopes } Date Range Input Prompts user for start and end dates, defaulting to the last 30 days Converts dates to proper format for Graph API queries function Get-ValidatedDate($prompt, $default) { do { $inDate = Read-Host $prompt if ([string]::IsNullOrWhiteSpace($inDate)) { return $default } $parsed = $null $valid = [datetime]::TryParseExact($inDate, 'yyyy-MM-dd', $null, 'None', [ref]$parsed) if ($valid) { return $parsed } Write-Warning \"Invalid date format. Please use yyyy-MM-dd.\" } while ($true) } # Prompt date range, defaulting to 30 days $startDate = Get-ValidatedDate \"Enter start date (yyyy-MM-dd), or press Enter for 30 days ago\" (Get-Date).AddDays(-30) $endDate = Get-ValidatedDate \"Enter end date (yyyy-MM-dd), or press Enter for today\" (Get-Date) # Format dates for Graph API $startDateUTC = [datetime]::Parse($startDate).ToUniversalTime().ToString(\"yyyy-MM-ddTHH:mm:ssZ\") $endDateUTC = ([datetime]::Parse($endDate).AddDays(1).AddSeconds(-1)).ToUniversalTime().ToString(\"yyyy-MM-ddTHH:mm:ssZ\") Date Collection Queries Microsoft Graph API for sign-in logs with the AzureAD PowerShell AppID Handles pagination for large result sets $aadAppId = '1b730954-1685-4b74-9bfd-dac224a7b894' $filter = \"appId eq '$aadAppId' and createdDateTime ge $startDateUTC and createdDateTime le $endDateUTC\" $uri = \"https://graph.microsoft.com/v1.0/auditLogs/signIns?`$filter=$( [uri]::EscapeDataString($filter) )\" Write-Host \"Retrieving sign-in logs for appId '$aadAppId' from $startDate to $endDate\" $allSignIns = @() do { try { $response = Invoke-MgGraphRequest -Uri $uri -Method GET } catch { Write-Error \"Failed to retrieve sign-in logs: $_\" Disconnect-MgGraph exit } $allSignIns += $response.value $uri = $response.'@odata.nextLink' } while ($uri) Data Processing Extracts relevant details from each sign-in (user, IP, location, status) Supports optional username filtering Formats data for reporting $progressCount = 0 $totalCount = $allSignIns.Count $results = $allSignIns | ForEach-Object { $progressCount++ # Display progress every 100 items if ($progressCount % 100 -eq 0) { $percentComplete = [math]::Round(($progressCount / $totalCount) * 100, 1) Write-Host \"Processing: $progressCount of $totalCount ($percentComplete%)\" -ForegroundColor Cyan } # Create PSCustomObject for each sign-in with the relevant properties [PSCustomObject]@{ # Extracted properties such as UserPrincipalName, CorrelationId } } # Apply username filter if specified if (-not [string]::IsNullOrWhiteSpace($usernameFilter)) { $results = $results | Where-Object { $_.UserPrincipalName -like \"*$usernameFilter*\" } } Report Generation Creates a CSV file for data analysis Generates an interactive HTML report with sortable columns Automatically opens the report based on your operating system # Export CSV $csvPath = Join-Path $PWD \"LegacyAzureAD_SignIns_Report.csv\" $results | Export-Csv -Path $csvPath -NoTypeInformation -Encoding UTF8 # Export HTML with interactive features $htmlPath = Join-Path $PWD \"LegacyAzureAD_SignIns_Report.html\" # HTML header with sorting JavaScript $htmlHeader = @\"&lt;style&gt; table {border-width: 1px; border-style: solid; border-color: black; border-collapse: collapse;} th {border-width: 1px; padding: 5px; border-style: solid; border-color: black; background-color: #0078D4; color: white; cursor: pointer;} td {border-width: 1px; padding: 5px; border-style: solid; border-color: black;} .sorted-asc::after { content: \" â–²\"; } .sorted-desc::after { content: \" â–¼\"; } tr:nth-child(even) {background-color: #f2f2f2;} tr:hover {background-color: #ddd;} &lt;/style&gt; &lt;script&gt; function sortTable(n) { // Sorting logic for table columns // ... } &lt;/script&gt; \"@ # Open HTML report based on platform try { if ($IsWindows -or $env:OS -match 'Windows') { Start-Process $htmlPath } elseif ($IsMacOS -or (uname) -eq 'Darwin') { Invoke-Expression \"open '$htmlPath'\" } elseif ($IsLinux -or (uname) -eq 'Linux') { Invoke-Expression \"xdg-open '$htmlPath'\" } } catch { Write-Warning \"Could not automatically open the report: $_\" } The Full PowerShell Script Hereâ€™s the complete PowerShell script, you can copy and save it or find the current version on GitHub at cocallaw/Entra-AAD-PS-Retirement. function Get-ValidatedDate($prompt, $default) { do { $inDate = Read-Host $prompt if ([string]::IsNullOrWhiteSpace($inDate)) { return $default } $parsed = $null $valid = [datetime]::TryParseExact($inDate, 'yyyy-MM-dd', $null, 'None', [ref]$parsed) if ($valid) { return $parsed } Write-Warning \"Invalid date format. Please use yyyy-MM-dd.\" } while ($true) } $InformationPreference = 'SilentlyContinue' $WarningPreference = 'SilentlyContinue' # Suppress warnings globally Write-Host \"Starting Legacy Azure AD PowerShell Sign-In Report script\" Write-Host \"Checking prerequisites\" # Check if Microsoft Graph commands are available if (-not (Get-Command Get-MgContext -ErrorAction SilentlyContinue)) { # Only then check if module is installed if (-not (Get-Module -ListAvailable -Name Microsoft.Graph)) { $WarningPreference = 'Continue' # Temporarily restore warnings for important messages Write-Warning \"Microsoft.Graph module is not installed. Please install it using: Install-Module Microsoft.Graph -Scope CurrentUser\" exit } # Module is installed but commands aren't available, so try to load it silently $oldWarningPreference = $WarningPreference $WarningPreference = 'SilentlyContinue' # Suppress warnings during module import try { # Try minimal import Import-Module Microsoft.Graph.Authentication -DisableNameChecking -Force -ErrorAction SilentlyContinue if (-not (Get-Command Get-MgContext -ErrorAction SilentlyContinue)) { # Try full module import with all error suppression flags Import-Module Microsoft.Graph -SkipEditionCheck -DisableNameChecking -Force -ErrorAction SilentlyContinue } } catch { # Silently ignore errors - we'll check command availability instead } $WarningPreference = $oldWarningPreference # Verify everything loaded correctly if (-not (Get-Command Get-MgContext -ErrorAction SilentlyContinue)) { Write-Error \"Failed to load Microsoft Graph commands. Please start with a fresh PowerShell session.\" exit 1 } } Write-Host \"Microsoft Graph commands are available\" $requiredScopes = @(\"AuditLog.Read.All\", \"Directory.Read.All\", \"IdentityRiskEvent.Read.All\") $needConnect = $false if (-not (Get-MgContext)) { Write-Host \"Connecting to Microsoft Graph...\" $needConnect = $true } else { $currentScopes = (Get-MgContext).Scopes if (-not $currentScopes -or ($requiredScopes | Where-Object { $_ -notin $currentScopes })) { Write-Host \"Existing session missing required scopes. Reconnecting...\" $needConnect = $true } else { Write-Host \"Already connected to Microsoft Graph. Reusing existing session.\" } } if ($needConnect) { Connect-MgGraph -Scopes $requiredScopes } # Prompt date range, defaulting to 30 days $startDate = Get-ValidatedDate \"Enter start date (yyyy-MM-dd), or press Enter for 30 days ago\" (Get-Date).AddDays(-30) $endDate = Get-ValidatedDate \"Enter end date (yyyy-MM-dd), or press Enter for today\" (Get-Date) # Prompt username filter (optional) $usernameFilter = Read-Host \"Enter username filter (optional), or press Enter to skip\" $aadAppId = '1b730954-1685-4b74-9bfd-dac224a7b894' $startDateUTC = [datetime]::Parse($startDate).ToUniversalTime().ToString(\"yyyy-MM-ddTHH:mm:ssZ\") $endDateUTC = ([datetime]::Parse($endDate).AddDays(1).AddSeconds(-1)).ToUniversalTime().ToString(\"yyyy-MM-ddTHH:mm:ssZ\") $filter = \"appId eq '$aadAppId' and createdDateTime ge $startDateUTC and createdDateTime le $endDateUTC\" $uri = \"https://graph.microsoft.com/v1.0/auditLogs/signIns?`$filter=$( [uri]::EscapeDataString($filter) )\" Write-Host \"Retrieving sign-in logs for appId '$aadAppId' from $startDate to $endDate\" $allSignIns = @() do { try { $response = Invoke-MgGraphRequest -Uri $uri -Method GET } catch { Write-Error \"Failed to retrieve sign-in logs: $_\" Disconnect-MgGraph exit } $allSignIns += $response.value $uri = $response.'@odata.nextLink' } while ($uri) if ($allSignIns.Count -eq 0) { Write-Warning \"No sign-in logs found for the specified filters.\" Disconnect-MgGraph exit } Write-Host \"Retrieved $($allSignIns.Count) sign-in logs for appId '$aadAppId' from $startDate to $endDate\" Write-Host \"Processing sign-in logs\" $progressCount = 0 $totalCount = $allSignIns.Count $results = $allSignIns | ForEach-Object { $progressCount++ # Display progress every 100 items if ($progressCount % 100 -eq 0) { $percentComplete = [math]::Round(($progressCount / $totalCount) * 100, 1) Write-Host \"Processing: $progressCount of $totalCount ($percentComplete%)\" -ForegroundColor Cyan } [PSCustomObject]@{ UserPrincipalName = $_.userPrincipalName AppDisplayName = $_.appDisplayName IPAddress = $_.ipAddress Location = (($_.location.city, $_.location.state, $_.location.countryOrRegion) -join \", \").Trim(', ') SignInTime = $_.createdDateTime Status = if ($_.status.errorCode -eq 0) { 'Success' } else { 'Failure' } RiskLevel = $_.riskLevelAggregated CorrelationId = $_.correlationId } } Write-Host \"Processing complete: $totalCount items processed\" -ForegroundColor Green # Apply username filter if specified if (-not [string]::IsNullOrWhiteSpace($usernameFilter)) { $results = $results | Where-Object { $_.UserPrincipalName -like \"*$usernameFilter*\" } } if ($results.Count -eq 0) { Write-Warning \"No sign-in logs match the username filter.\" Disconnect-MgGraph exit } # Export CSV $csvPath = Join-Path $PWD \"LegacyAzureAD_SignIns_Report.csv\" try { $results | Export-Csv -Path $csvPath -NoTypeInformation -Encoding UTF8 Write-Host \"CSV report saved to: $csvPath\" } catch { Write-Error \"Failed to export CSV file: $_\" } # Export HTML $htmlPath = Join-Path $PWD \"LegacyAzureAD_SignIns_Report.html\" $htmlHeader = @\" &lt;style&gt; table {border-width: 1px; border-style: solid; border-color: black; border-collapse: collapse;} th {border-width: 1px; padding: 5px; border-style: solid; border-color: black; background-color: #0078D4; color: white; cursor: pointer;} td {border-width: 1px; padding: 5px; border-style: solid; border-color: black;} .sorted-asc::after { content: \" â–²\"; } .sorted-desc::after { content: \" â–¼\"; } tr:nth-child(even) {background-color: #f2f2f2;} tr:hover {background-color: #ddd;} &lt;/style&gt; &lt;title&gt;Legacy Azure AD PowerShell Sign-In Report - $($startDate.ToString('yyyy-MM-dd')) to $($endDate.ToString('yyyy-MM-dd'))&lt;/title&gt; &lt;script&gt; function sortTable(n) { var table, rows, switching, i, x, y, shouldSwitch, dir, switchcount = 0; table = document.getElementsByTagName(\"table\")[0]; switching = true; // Set the sorting direction to ascending: dir = \"asc\"; // Remove sort indicators from all headers var headers = table.getElementsByTagName(\"th\"); for (i = 0; i &lt; headers.length; i++) { headers[i].classList.remove(\"sorted-asc\", \"sorted-desc\"); } /* Make a loop that will continue until no switching has been done: */ while (switching) { switching = false; rows = table.rows; /* Loop through all table rows (except the first, which contains table headers): */ for (i = 1; i &lt; (rows.length - 1); i++) { shouldSwitch = false; /* Get the two elements you want to compare, one from current row and one from the next: */ x = rows[i].getElementsByTagName(\"TD\")[n]; y = rows[i + 1].getElementsByTagName(\"TD\")[n]; if (!x || !y) continue; // Check if the two rows should switch place: if (dir == \"asc\") { if (isDate(x.innerHTML) &amp;&amp; isDate(y.innerHTML)) { if (new Date(x.innerHTML) &gt; new Date(y.innerHTML)) { shouldSwitch = true; break; } } else { if (x.innerHTML.toLowerCase() &gt; y.innerHTML.toLowerCase()) { shouldSwitch = true; break; } } } else if (dir == \"desc\") { if (isDate(x.innerHTML) &amp;&amp; isDate(y.innerHTML)) { if (new Date(x.innerHTML) &lt; new Date(y.innerHTML)) { shouldSwitch = true; break; } } else { if (x.innerHTML.toLowerCase() &lt; y.innerHTML.toLowerCase()) { shouldSwitch = true; break; } } } } if (shouldSwitch) { rows[i].parentNode.insertBefore(rows[i + 1], rows[i]); switching = true; switchcount++; } else { if (switchcount == 0 &amp;&amp; dir == \"asc\") { dir = \"desc\"; switching = true; } } } // Add sort indicator to the clicked header headers[n].classList.add(dir === \"asc\" ? \"sorted-asc\" : \"sorted-desc\"); } // Helper function to detect dates function isDate(value) { // Simple date detection for common formats const datePattern = /^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}|^\\d{1,2}\\/\\d{1,2}\\/\\d{4}/; return datePattern.test(value); } &lt;/script&gt; &lt;h1&gt;Legacy Azure AD PowerShell Sign-In Report&lt;/h1&gt; &lt;p&gt;Report generated: $(Get-Date -Format \"yyyy-MM-dd HH:mm\")&lt;/p&gt; &lt;p&gt;Date range: $($startDate.ToString('yyyy-MM-dd')) to $($endDate.ToString('yyyy-MM-dd'))&lt;/p&gt; \"@ # Generate HTML without the replacement logic first $preContent = $htmlHeader $postContent = \"&lt;p&gt;&lt;em&gt;Click on column headers to sort&lt;/em&gt;&lt;/p&gt;\" # Generate basic HTML $html = $results | ConvertTo-Html -Property UserPrincipalName, AppDisplayName, IPAddress, Location, SignInTime, Status, RiskLevel, CorrelationId -PreContent $preContent -PostContent $postContent # Create a more reliable method to replace the header names and add onclick handlers $columnOrder = @('UserPrincipalName', 'AppDisplayName', 'IPAddress', 'Location', 'SignInTime', 'Status', 'RiskLevel', 'CorrelationId') $headerDisplayNames = @{ 'UserPrincipalName' = 'User Principal Name' 'AppDisplayName' = 'App Display Name' 'IPAddress' = 'IP Address' 'Location' = 'Location' 'SignInTime' = 'Sign-In Time' 'Status' = 'Status' 'RiskLevel' = 'Risk Level' 'CorrelationId' = 'Correlation ID' } # Process the header row directly using a more reliable HTML parsing approach for ($i = 0; $i -lt $columnOrder.Count; $i++) { $colName = $columnOrder[$i] $displayName = $headerDisplayNames[$colName] $oldHeader = \"&lt;th&gt;$colName&lt;/th&gt;\" $newHeader = \"&lt;th onclick=\"\"sortTable($i)\"\"&gt;$displayName&lt;/th&gt;\" $html = $html -replace [regex]::Escape($oldHeader), $newHeader } # Save the HTML file $html | Set-Content -Path $htmlPath Write-Host \"HTML report saved to: $htmlPath\" # Open HTML report based on platform try { if ($IsWindows -or $env:OS -match 'Windows') { Start-Process $htmlPath } elseif ($IsMacOS -or (uname) -eq 'Darwin') { Invoke-Expression \"open '$htmlPath'\" } elseif ($IsLinux -or (uname) -eq 'Linux') { Invoke-Expression \"xdg-open '$htmlPath'\" } else { Write-Information \"HTML report saved to: $htmlPath (please open manually)\" } } catch { Write-Warning \"Could not automatically open the report: $_\" Write-Host \"HTML report saved to: $htmlPath (please open manually)\" } Final Recommendations Identify remaining users/scripts using the legacy modules now. Continue to use sign-in log reports to monitor usage burn down or unauthorized usage leading up to the future retirement date. Begin migrating those to Microsoft Graph PowerShell or Microsoft Entra PowerShell. Additional Resources PowerShell Gallery: AzureAD Module Microsoft Graph PowerShell SDK Microsoft Entra PowerShell." }, { "title": "Export BitLocker Keys from Entra with Graph PowerShell", "url": "/posts/Export-Bitlocker-Keys-from-Entra/", "categories": "PowerShell, Azure, Microsoft Graph", "tags": "PowerShell, Azure, CrowdStrike, BitLocker, Entra, Microsoft Graph", "date": "2025-05-12 06:00:00 -0400", "content": "On the day of the CrowdStrike outage, countless Windows devices across the world became unstableâ€”many of them failing to boot or respond. For some machines administrators needed one critical thing to be able to access the disk and attempt repair: the BitLocker recovery key. If the Windows machine was registered with Intune/Entra, these recovery keys were likely stored securely in your tenant. Working with different customers and the help of PowerShell and Microsoft Graph, a script was created over that weekend that was able to retrieve BitLocker Keys quickly and at scale so that they could be provided to whatever the next step was in the recovery process. The Script: cocallaw/Entra-Get-BLKeys The Problem Windows machines were unable to boot due to a bad kernel driver. To recovery, admins needed to access the disk and remove the problematic driver. If BitLocker encryption was enabled on the OS disk, the machine would require a recovery key to unlock the drive. Admins needed the recovery keys for numerous machines, but they were not easily accessible at scale. The Solution: PowerShell + Microsoft Graph The following PowerShell script automates the retrieval of BitLocker recovery keys for a list of machine names. It queries Microsoft Graph to find devices by name, then pulls their associated keys. Pre-req: The machine must be enrolled in Intune/Entra The account connecting must have Device.Read.All and BitLockerKey.Read.All permissions in Microsoft Graph. A CSV file with the one column named MachineName and the names of machines in the column. Running the Script Save the script as Get-BitLockerKeys.ps1. Open PowerShell and run the script with the path to your CSV file as the parameter -MachinesCSV. The script will output a CSV file named DeviceRecoveryKeys.csv. .\\Get-BitLockerKeys.ps1 -MachinesCSV \"C:\\path\\to\\your\\machines.csv\" Section 1: Input Validation The first part ensures that the file provided by the MachinesCSV parameter exists, is a .csv, and contains the expected header. Param ( [string]$MachinesCSV ) if (-not (Test-Path -Path $MachinesCSV)) { Write-Error \"The file '$MachinesCSV' does not exist. Please provide a valid file path.\" exit } if (-not ($MachinesCSV -like \"*.csv\")) { Write-Error \"The file '$MachinesCSV' is not a CSV file.\" exit } $csvContent = Import-Csv -Path $MachinesCSV if (-not $csvContent[0].PSObject.Properties.Name -contains \"MachineName\") { Write-Error \"The CSV must contain a 'MachineName' column.\" exit } Section 2: Module Check &amp; Authentication This section checks that the Microsoft Graph module is available and current, then connects with the appropriate scopes. $module = Get-Module -ListAvailable -Name Microsoft.Graph if (-not $module) { Write-Error \"Microsoft Graph PowerShell module is not installed.\" exit } elseif ($module.Version -lt [Version]\"2.5.0\") { Write-Error \"Please update the Microsoft Graph module.\" exit } try { Connect-MgGraph -Scopes \"Device.Read.All\", \"BitLockerKey.Read.All\" -NoWelcome } catch { Write-Error \"Failed to connect to Microsoft Graph.\" exit } Import-Module Microsoft.Graph.Identity.DirectoryManagement -Force Section 3: Device Lookup Once connected this part queries Microsoft Graph to find matching device records by names as listed in the CSV file provided. $vmNames = (Import-Csv -Path $MachinesCSV).MachineName $devices = @() foreach ($vmName in $vmNames) { $devices += Get-MgDevice -Filter \"displayName eq '$vmName'\" } Section 4: Retrieve BitLocker Keys With the list of machines stored in $devices, we pull recovery keys per device and if no recovery key info is found set NoKeyFound as the value for the Recovery Key and ID. $deviceRecoveryKeys = @() foreach ($device in $devices) { $recoveryKeys = Get-MgInformationProtectionBitlockerRecoveryKey -Filter \"deviceId eq '$($device.DeviceId)'\" if ($recoveryKeys) { foreach ($key in $recoveryKeys) { $deviceRecoveryKeys += [PSCustomObject]@{ DeviceName = $device.DisplayName DeviceId = $device.DeviceId RecoveryKeyId = $key.Id RecoveryKey = (Get-MgInformationProtectionBitlockerRecoveryKey -BitlockerRecoveryKeyId $key.Id -Property key).key } } } else { $deviceRecoveryKeys += [PSCustomObject]@{ DeviceName = $device.DisplayName DeviceId = $device.DeviceId RecoveryKeyId = \"NoKeyFound\" RecoveryKey = \"NoKeyFound\" } } } Section 5: Export to CSV Finally, the key information is output to a CSV file named DeviceRecoveryKeys.csv and displays the path. $outputFilePath = \"DeviceRecoveryKeys.csv\" $deviceRecoveryKeys | Export-Csv -Path $outputFilePath -NoTypeInformation $absolutePath = Resolve-Path -Path $outputFilePath Write-Output \"Device recovery keys have been exported to '$absolutePath'.\" Section 6: Sample Output The output CSV DeviceRecoveryKeys.csv will follow this format: \"DeviceName\",\"DeviceId\",\"RecoveryKeyId\",\"RecoveryKey\" \"test-vm-0\",\"90e61314-d6dd-4ff8-a878-754b5b60a8be\",\"11135913-8395-458b-a7bf-dd7ef10214a2\",\"375177-019569-186362-280841-610973-240801-321277-046255\" \"test-vm-1\",\"c7165ba4-0fac-4ec5-82bf-5e4ff05dcdc8\",\"NoKeyFound\",\"NoKeyFound\" Full Script For easy copy/pasting, the entire script is below in one block. You can also find it in the GitHub Repo cocallaw/Entra-Get-BLKeys. Param ( [string]$MachinesCSV ) if (-not (Test-Path -Path $MachinesCSV)) { Write-Error \"The file '$MachinesCSV' does not exist. Please provide a valid file path.\" exit } if (-not ($MachinesCSV -like \"*.csv\")) { Write-Error \"The file '$MachinesCSV' is not a CSV file.\" exit } $csvContent = Import-Csv -Path $MachinesCSV if (-not $csvContent[0].PSObject.Properties.Name -contains \"MachineName\") { Write-Error \"The CSV must contain a 'MachineName' column.\" exit } $module = Get-Module -ListAvailable -Name Microsoft.Graph if (-not $module) { Write-Error \"Microsoft Graph PowerShell module is not installed.\" exit } elseif ($module.Version -lt [Version]\"2.5.0\") { Write-Error \"Please update the Microsoft Graph module.\" exit } try { Connect-MgGraph -Scopes \"Device.Read.All\", \"BitLockerKey.Read.All\" -NoWelcome } catch { Write-Error \"Failed to connect to Microsoft Graph.\" exit } Import-Module Microsoft.Graph.Identity.DirectoryManagement -Force $vmNames = (Import-Csv -Path $MachinesCSV).MachineName $devices = @() foreach ($vmName in $vmNames) { $devices += Get-MgDevice -Filter \"displayName eq '$vmName'\" } $deviceRecoveryKeys = @() foreach ($device in $devices) { $recoveryKeys = Get-MgInformationProtectionBitlockerRecoveryKey -Filter \"deviceId eq '$($device.DeviceId)'\" if ($recoveryKeys) { foreach ($key in $recoveryKeys) { $deviceRecoveryKeys += [PSCustomObject]@{ DeviceName = $device.DisplayName DeviceId = $device.DeviceId RecoveryKeyId = $key.Id RecoveryKey = (Get-MgInformationProtectionBitlockerRecoveryKey -BitlockerRecoveryKeyId $key.Id -Property key).key } } } else { $deviceRecoveryKeys += [PSCustomObject]@{ DeviceName = $device.DisplayName DeviceId = $device.DeviceId RecoveryKeyId = \"NoKeyFound\" RecoveryKey = \"NoKeyFound\" } } } $outputFilePath = \"DeviceRecoveryKeys.csv\" $deviceRecoveryKeys | Export-Csv -Path $outputFilePath -NoTypeInformation $absolutePath = Resolve-Path -Path $outputFilePath Write-Output \"Device recovery keys have been exported to '$absolutePath'.\" Permissions Reminder This script requires delegated or app permissions for: Device.Read.All BitLockerKey.Read.All Notes Recovery keys are only available if the device was properly enrolled and reporting compliance. Permissions must be delegated or granted to the appropriate Graph scopes The script exports the keys in plaintext, be cautious with the output file or update the script to output in a more secure manner This script is designed for educational purposes and should be tested in a safe environment before use in production. Final Thoughts This was a good reminder that recovery isnâ€™t just about backupsâ€”itâ€™s also about access. PowerShell and Microsoft Graph made it possible to recover quickly during the CrowdStrike outage by tapping into the organizationâ€™s existing processes and tooling. The script is a great example of how to leverage existing tools to solve real-world problems. If you have any questions or need help with the script, feel free to reach out." }, { "title": "Automating Container Builds with GitHub Actions for Upstream Changes", "url": "/posts/Automating-Container-Builds-for-Upstream-Changes/", "categories": "GitHub Actions, Docker, Automation, DevOps", "tags": "DevOps, Automation", "date": "2025-05-05 09:00:00 -0400", "content": "Keeping your container images up to date is crucial for security and performance. Whether youâ€™re running a homelab or managing production workloads, you want to ensure that your containers are built with the latest upstream changes. However, manually keeping track of updates and rebuilding containers can be tedious. I use Tailscale to easily access services I run at home, or to easily connect to a lab environment in Azure from wherever I may be. To make this easier, I extended the Tailscale container image so I could deploy it as a subnet router and inject the address ranges that I needed advertised to my Tailnet. This made my setup more resilient, but I still had to make sure my container was using the latest version of Tailscale to make sure I was secure and prevent possible issues. Until recently, I had a simple GitHub Actions workflow that would build and push to Docker Hub, whenever I remembered to check for updates (which I didnâ€™t do very regularly) to the upstream Tailscale image. In this post, Iâ€™ll walk through how I use a GitHub Actions workflow to automatically check for updates to an upstream Docker image, and trigger a build and push only if a new version is available. This kind of automation saves time, reduces human error, and keeps my environment reliably up to date. Why Automate Container Builds? If youâ€™re maintaining a custom container that wraps or extends functionality from an upstream project (like Tailscale, Nginx, or Node.js), youâ€™ll often want to: Track upstream releases. Rebuild your container with the new version. Push the updated image to Docker Hub or a private registry. Optionally commit a version change back to your repo for tracking. Doing all this manually is inefficient. Automating this with GitHub Actions ensures consistency and frees you to focus on more important tasks. The Workflow Hereâ€™s a breakdown of the GitHub Actions workflow I use. It: Runs every 3 days or on manual trigger. Checks the latest stable tag of the upstream container. Compares it against images already published. If a newer version is available, builds and pushes multi-arch images. Commits a version file update for visibility. Secrets You Will Need Before you can use this workflow, youâ€™ll need to set up a few secrets in your GitHub repository to allow the workflow to authenticate with Docker Hub. The workflow uses a Docker Hub username and an access token for authentication. DOCKERHUB_USERNAME: Your Docker Hub username. DOCKERHUB_TOKEN: A Docker Hub access token with write permissions. To set these secrets, go to your GitHub repository, click on Settings, then Secrets and variables &gt; Actions. Click on New repository secret to add each secret. Workflow Breakdown Letâ€™s walk through what each section of the workflow does and how it contributes to the build and update process. 1. Triggers on: schedule: - cron: '0 0 */3 * *' workflow_dispatch: schedule: Runs the workflow automatically every 3 days. workflow_dispatch: Allows you to manually trigger the workflow from the GitHub UI. 2. Permissions permissions: contents: write This grants the workflow permission to make commits to the repository, which is necessary so that the upstream-version.txt file can be updated and pushed to the repo if there is a new upstream container version. 3. Job: check-and-build The entire automation is contained in a single job that runs using the latest Ubuntu GitHub Actions runner. Step: Checkout Repo jobs: check-and-build: runs-on: ubuntu-latest steps: - name: Checkout repo uses: actions/checkout@v3 This pulls down the repository code so that subsequent steps can read/write files like upstream-version.txt and the Dockerfile. Step: Get Latest Upstream Version - name: Get latest Tailscale stable version id: tailscale run: | VERSION=$(curl -s https://registry.hub.docker.com/v2/repositories/tailscale/tailscale/tags/?page_size=100 \\ | jq -r '.results[].name' \\ | grep -E '^v[0-9]+\\.[0-9]+(\\.[0-9]+)?$' \\ | sort -V \\ | tail -n1) echo \"Latest stable version: $VERSION\" echo \"version=$VERSION\" &gt;&gt; \"$GITHUB_OUTPUT\" - name: Debug version run: | echo \"Detected Tailscale version: ${{ steps.tailscale.outputs.version }}\" Uses Docker Hubâ€™s API to fetch the latest stable tag from an upstream image. Outputs the latest version as steps.upstream.outputs.version. Step: Check for Existing Image - name: Check if image with version already exists id: check run: | if docker manifest inspect ${{ secrets.DOCKERHUB_USERNAME }}/tailscale-sr:${{ steps.tailscale.outputs.version }} &gt; /dev/null 2&gt;&amp;1; then echo \"Image already exists. Skipping build.\" echo \"build_needed=false\" &gt;&gt; $GITHUB_OUTPUT else echo \"New version. Proceeding to build.\" echo \"build_needed=true\" &gt;&gt; $GITHUB_OUTPUT fi Uses docker manifest inspect to check whether a container image for this version already exists in your Docker Hub repo. Sets a flag build_needed to true or false. Conditional Build Steps The following steps are only run if a new version is detected, resulting in build_needed being set to true. Set up emulation support (QEMU) for building multi-architecture containers. Set up Docker Buildx for advanced build features. Log into Docker Hub using credentials stored in GitHub Secrets. Build and push the container using the new version as a tag. - name: Set up QEMU if: steps.check.outputs.build_needed == 'true' uses: docker/setup-qemu-action@v2 - name: Set up Docker Buildx if: steps.check.outputs.build_needed == 'true' uses: docker/setup-buildx-action@v2 - name: Login to DockerHub if: steps.check.outputs.build_needed == 'true' uses: docker/login-action@v2 with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} - name: Build and push multi-arch container if: steps.check.outputs.build_needed == 'true' uses: docker/build-push-action@v3 with: context: ./docker build-args: | TAILSCALE_TAG=${{ steps.tailscale.outputs.version }} platforms: linux/amd64,linux/arm64 push: true tags: | ${{ secrets.DOCKERHUB_USERNAME }}/tailscale-sr:latest ${{ secrets.DOCKERHUB_USERNAME }}/tailscale-sr:${{ steps.tailscale.outputs.version }} Step: Commit Version Update - name: Commit updated version file if: steps.check.outputs.build_needed == 'true' run: | echo \"${{ steps.tailscale.outputs.version }}\" &gt; tailscale-version.txt git config user.name \"github-actions[bot]\" git config user.email \"github-actions[bot]@users.noreply.github.com\" git add tailscale-version.txt git commit -m \"Update Tailscale version to ${{ steps.tailscale.outputs.version }}\" git push This writes the new version to a file, commits it, and pushes it to your GitHub repo. This gives you a version history and a way to confirm that the build occurred. The YAML Workflow name: Auto Build &amp; Publish Tailscale Container on: schedule: - cron: '0 0 */3 * *' # Every 3 days at 00:00 UTC workflow_dispatch: permissions: contents: write # Needed to commit version file jobs: check-and-build: runs-on: ubuntu-latest steps: - name: Checkout repo uses: actions/checkout@v3 - name: Get latest Tailscale stable version id: tailscale run: | VERSION=$(curl -s https://registry.hub.docker.com/v2/repositories/tailscale/tailscale/tags/?page_size=100 \\ | jq -r '.results[].name' \\ | grep -E '^v[0-9]+\\.[0-9]+(\\.[0-9]+)?$' \\ | sort -V \\ | tail -n1) echo \"Latest stable version: $VERSION\" echo \"version=$VERSION\" &gt;&gt; \"$GITHUB_OUTPUT\" - name: Debug version run: | echo \"Detected Tailscale version: ${{ steps.tailscale.outputs.version }}\" - name: Check if image with version already exists id: check run: | if docker manifest inspect ${{ secrets.DOCKERHUB_USERNAME }}/tailscale-sr:${{ steps.tailscale.outputs.version }} &gt; /dev/null 2&gt;&amp;1; then echo \"Image already exists. Skipping build.\" echo \"build_needed=false\" &gt;&gt; $GITHUB_OUTPUT else echo \"New version. Proceeding to build.\" echo \"build_needed=true\" &gt;&gt; $GITHUB_OUTPUT fi - name: Set up QEMU if: steps.check.outputs.build_needed == 'true' uses: docker/setup-qemu-action@v2 - name: Set up Docker Buildx if: steps.check.outputs.build_needed == 'true' uses: docker/setup-buildx-action@v2 - name: Login to DockerHub if: steps.check.outputs.build_needed == 'true' uses: docker/login-action@v2 with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} - name: Build and push multi-arch container if: steps.check.outputs.build_needed == 'true' uses: docker/build-push-action@v3 with: context: ./docker build-args: | TAILSCALE_TAG=${{ steps.tailscale.outputs.version }} platforms: linux/amd64,linux/arm64 push: true tags: | ${{ secrets.DOCKERHUB_USERNAME }}/tailscale-sr:latest ${{ secrets.DOCKERHUB_USERNAME }}/tailscale-sr:${{ steps.tailscale.outputs.version }} - name: Commit updated version file if: steps.check.outputs.build_needed == 'true' run: | echo \"${{ steps.tailscale.outputs.version }}\" &gt; tailscale-version.txt git config user.name \"github-actions[bot]\" git config user.email \"github-actions[bot]@users.noreply.github.com\" git add tailscale-version.txt git commit -m \"Update Tailscale version to ${{ steps.tailscale.outputs.version }}\" git push Final Thoughts Automating container builds with GitHub Actions is a powerful way to keep your images up to date without manual intervention. This workflow can be adapted for any upstream project, and you can customize it further based on your needs. Consider adding notifications (e.g., Slack, email) to alert you when a new version is built. You can also extend the workflow to run tests against the new image before pushing it. Explore other GitHub Actions to enhance your CI/CD pipeline. If youâ€™ve built similar automationâ€™s or have tips for improving this setup, let me knowâ€”Iâ€™d love to hear how others are tackling this." }, { "title": "Create Confidential Compute Capable Custom Images from Windows CVMs", "url": "/posts/Creating-Confidentail-Compute-Images-from-CVMs/", "categories": "Confidential Compute", "tags": "images", "date": "2025-01-06 12:00:00 -0500", "content": "Confidential Compute Custom Images Creating a custom image from an Azure Confidential Compute VM (CVM) requires following a different process than you would for a regular Azure VM. This is due to the design of CVMs, which utilize an OS disk and a small encrypted data disk that contains the VM Guest State(VMGS) information. As a result, using the Capture button in the Azure portal or the New-AzImage command in Azure PowerShell will not produce the desired results. This process to create a custom image from a CVM is necessary so that the captured image is free of and VM Guest State information and the correct properties are set for the image version and reference in a Azure Compute Gallery. The steps outlined below require that you have access to the Azure Subscriptions containing the resources, and the current version of both Azure CLI and AzCopy installed. Commands were tested using Azure CLI from a Powershell Core session. This process to create a custom image is for an existing Windows based CVM using with Confidential OS disk encryption enabled using either PMK or CMK. Prepare the CVM OS for Capture Once the customization of the Windows OS is complete, the next step is to Disable BitLocker, wait for the decryption to complete, and then run Sysprep. To disable BitLocker and check the decryption status of the OS disk, you can use the following commands in an elevated Command Prompt. # Disable BitLocker manage-bde -off C: # Check the decryption status manage-bde -status C: When the decryption status returns as Fully Decrypted, Sysprep can now be run. Selecting Generalize and Shutdown as the options. Creating the Custom Image Collect OS Disk Information The first step is to make sure the CVM the image is being created from is fully deallocated and then collect information about the OS disk. To do so we will need to know the resource group name, VM name, and region that the CVM is located in, we will set these as variables for easy reference. $region = \"North Europe\" $resourceGroupName = \"rg-custimg-lab-01\" $vmName = \"custcvm-01\" With the variables set, verify the VM is deallocated # Deallocate the VM az vm deallocate --name $vmname --resource-group $resourceGroupName # Collect the OS Disk information $disk_name = (az vm show --name $vmname --resource-group $resourceGroupName | jq -r .storageProfile.osDisk.name) $disk_url = (az disk grant-access --duration-in-seconds 3600 --name $disk_name --resource-group $resourceGroupName | jq -r .accessSas) Create a Storage Account for the VHD Next, create a storage account, this will be used to store the exported VHD of the CVMs OS disk before it is uploaded to the Compute Gallery. For this part of the process, you will need to know the name of the Storage Account and Container that will be created. $storageAccountName = \"stgcvmvhd01\" $storageContainerName = \"cvmimages\" $referenceVHD = \"${vmName}.vhd\" Create the Storage Account and Container # Create Storage Account az storage account create --resource-group ${resourceGroupName} --name ${storageAccountName} --location $region --sku \"Standard_LRS\" # Create a container in the Storage Account az storage container create --name $storageContainerName --account-name $storageAccountName --resource-group $resourceGroupName With the Storage Account and Container created, generate a Shared Access Signature (SAS) token to upload the disk image to the container. Be sure to set the expiry date to a date in the future. # Generate a SAS token for the container $container_sas=(az storage container generate-sas --name $storageContainerName --account-name $storageAccountName --auth-mode key --expiry 2025-01-01 --https-only --permissions dlrw -o tsv) Using the SAS token and information collected, the VHD can now be exported to the Storage Account using AzCopy. # Build the Blob URL $blob_url=\"https://${storageAccountName}.blob.core.windows.net/$storageContainerName/$referenceVHD\" # Export the VHD using AzCopy azcopy copy \"$disk_url\" \"${blob_url}?${container_sas}\" Upload the Image to a Compute Gallery With the VHD successfully exported to the Storage Account, the next step is to create an image definition in the Compute Gallery and upload the VHD to the gallery as a new version. For this part of the process, you will need to know the name of the Compute Gallery, Image Definition, Offer, Publisher, SKU, and Version number that will be created. $galleryName = \"acglabneu01\" $imageDefinitionName = \"cvmimage01\" $OfferName = \"offername01\" $PublisherName = \"pubname01\" $SkuName = \"skuname01\" $galleryImageVersion = \"1.0.0\" If a Compute Gallery does not already exist, create one using the following command, and create an Image Definition that has the required features and parameters set for Confidential VM support. # Create the Compute Gallery az sig create --resource-group $resourceGroupName --gallery-name $galleryName # Create the Image Definition az sig image-definition create --resource-group $resourceGroupName --location $region --gallery-name $galleryName --gallery-image-definition $imageDefinitionName --publisher $PublisherName --offer $OfferName --sku $SkuName --os-type windows --os-state Generalized --hyper-v-generation V2 --features SecurityType=ConfidentialVMSupported To upload the VHD to the Compute Gallery, the ID of the Storage Account that contains tehVHD is required when creating the image version. This can be obtained using the following command. # Get the Storage Account ID $storageAccountId=(az storage account show --name $storageAccountName --resource-group $resourceGroupName | jq -r .id) With everything in place, the final step is to create the image version in the Compute Gallery using the VHD that was exported from the CVM. # Create the Image Version az sig image-version create --resource-group $resourceGroupName --gallery-name $galleryName --gallery-image-definition $imageDefinitionName --gallery-image-version $galleryImageVersion --os-vhd-storage-account $storageAccountId --os-vhd-uri $blob_url The Full Image Export Process Below is the full process to export the OS disk from the CVM, create the image, and upload it to the Compute Gallery. # Set Variables $region = \"North Europe\" $resourceGroupName = \"rg-custimg-lab-01\" $vmName = \"custcvm-01\" $storageAccountName = \"stgcvmvhd01\" $storageContainerName = \"cvmimages\" $referenceVHD = \"${vmName}.vhd\" $galleryName = \"acglabneu01\" $imageDefinitionName = \"cvmimage01\" $OfferName = \"offername01\" $PublisherName = \"pubname01\" $SkuName = \"skuname01\" $galleryImageVersion = \"1.0.0\" # Deallocate the VM az vm deallocate --name $vmName --resource-group $resourceGroupName # Collect the OS Disk information $disk_name = (az vm show --name $vmName --resource-group $resourceGroupName | jq -r .storageProfile.osDisk.name) $disk_url = (az disk grant-access --duration-in-seconds 3600 --name $disk_name --resource-group $resourceGroupName | jq -r .accessSas) # Create Storage Account az storage account create --resource-group ${resourceGroupName} --name ${storageAccountName} --location $region --sku \"Standard_LRS\" # Create a container in the Storage Account az storage container create --name $storageContainerName --account-name $storageAccountName --resource-group $resourceGroupName # Generate a SAS token for the container $container_sas=(az storage container generate-sas --name $storageContainerName --account-name $storageAccountName --auth-mode key --expiry 2025-01-01 --https-only --permissions dlrw -o tsv) # Build the Blob URL $blob_url=\"https://${storageAccountName}.blob.core.windows.net/$storageContainerName/$referenceVHD\" # Export the VHD using AzCopy azcopy copy \"$disk_url\" \"${blob_url}?${container_sas}\" # Create the Compute Gallery az sig create --resource-group $resourceGroupName --gallery-name $galleryName # Create the Image Definition az sig image-definition create --resource-group $resourceGroupName --location $region --gallery-name $galleryName --gallery-image-definition $imageDefinitionName --publisher $PublisherName --offer $OfferName --sku $SkuName --os-type windows --os-state Generalized --hyper-v-generation V2 --features SecurityType=ConfidentialVMSupported # Get the Storage Account ID $storageAccountId=(az storage account show --name $storageAccountName --resource-group $resourceGroupName | jq -r .id) # Create the Image Version az sig image-version create --resource-group $resourceGroupName --gallery-name $galleryName --gallery-image-definition $imageDefinitionName --gallery-image-version $galleryImageVersion --os-vhd-storage-account $storageAccountId --os-vhd-uri $blob_url" }, { "title": "Creating Confidential Compute Capable Custom Images from Standard Windows VMs", "url": "/posts/Creating-Confidential-Compute-Capable-Custom-Images/", "categories": "Confidential Compute", "tags": "images", "date": "2025-01-05 12:00:00 -0500", "content": "Confidential Compute and Custom Images Creating a custom image that can be used to create Azure Confidential Compute (ACC) VMs is similar to creating a standard custom image, but with a slight twist when it comes to how the image is captured. This post covers how to create a custom image that could be used to provision a new ACC VM using either Customer Managed Key (CMK) or Platform Managed Key (PMK) encryption in any Azure region that has ACC capable AMD VM SKUs. It is important to use the proper settings when creating the VM used to generate the custom image and capture the custom image, so that settings and features such as BitLocker are not enabled, which would require additional steps before capturing and possibly cause issues when using the captured image to provision a new CVM. The steps outlined below require that you have access to the Azure Subscriptions containing the resources, and the current version of Azure PowerShell installed. Commands were tested using Azure PowerShell from a Powershell Core session. Creating the VM for Custom Image Capture From the Azure Portal, Select Virtual Machine and Create New VM On the Instance Detail page set Security Type to Standard, and select a non-ACC VM SKU On the Disks tab, set Key Management to Platform-managed Key and leave Encryption at host unchecked Once the Custom Image VM has been deployed, connect to the machine and perform any customization tasks required. When all customizations are complete, run Sysprep with OOBE, Generalize and Shutdown selected Once Sysprep has completed, the VM is ready to be captured Capture the Custom Image From an Azure PowerShell session connected to the Subscription that contains the Custom Image VM, set the proper values for the variables $vmName, $rgName, $location and $imageName. $vmName = \"customvm01\" $rgNameCustImg = \"rg-custom-img-01\" $location = \"North Europe\" $imageName = \"image-01\" With the variables set run the following commands to create a Manged Image resource from the OS disk of the Custom Image VM. # Verify that the VM is deallocated Stop-AzVM -ResourceGroupName $rgNameCustImg -Name $vmName -Force # Set the status of the VM to generalized Set-AzVm -ResourceGroupName $rgNameCustImg -Name $vmName -Generalized # Store the VM details in a variable $vm = Get-AzVM -Name $vmName -ResourceGroupName $rgNameCustImg # Create the image configuration $imageConfig = New-AzImageConfig -Location $location -SourceVirtualMachineId $vm.Id -HyperVGeneration V2 # Create an image from the VM New-AzImage -ImageName $imageName -ResourceGroupName $rgNameCustImg -Image $imageConfig Importing the Custom Image into Azure Compute Gallery Azure Portal In the Azure Compute Gallery create a new Image Definition, with the Security Type set to Trusted launch and confidential VM supported On the next page select the Managed Image Resource to import to the Compute Gallery for the Image Definition Azure PowerShell To import the Managed Image into the Compute Gallery, there first needs to be a new Image Definition created and then the Managed Image imported as a new version of the image definition. To create the new Image Definition, set the following variables $rgNameACG = \"rg-compute-gallery-01\" $location = \"North Europe\" $galleryName = \"acgallery01\" $galleryImageDefinitionName = \"Def01\" $publisherName = \"Publisher01\" $offerName = \"Offer01\" $skuName = \"Win11-24H2\" $description = \"Windows 11 24H2\" # Variables to set the features of the Image Definition $ConfidentialVMSupported = @{Name='SecurityType';Value='TrustedLaunchAndConfidentialVmSupported'} $IsHibernateSupported = @{Name='IsHibernateSupported';Value='False'} $features = @($ConfidentialVMSupported,$IsHibernateSupported) With the variables set, run the following command to create the new Image Definition in the Compute Gallery New-AzGalleryImageDefinition -ResourceGroupName $rgNameACG -GalleryName $galleryName -Name $galleryImageDefinitionName -Location $location -Publisher $publisherName -Offer $offerName -Sku $skuName -OsState \"Generalized\" -OsType \"Windows\" -Description $description -Feature $features -HyperVGeneration \"V2\" Now that there is an Image Definition in the Compute Gallery, the Managed Image that was created from the Custom Image VM can be imported as a version of the Image Definition. To do this, first the Resource ID of the Managed Image needs to be retrieved. $imageRID = (Get-AzImage -ResourceGroupName $rgNameCustImg -ImageName $imageName).Id $rgNameACG = \"rg-compute-gallery-01\" $location = \"North Europe\" $galleryName = \"acgallery01\" $galleryImageDefinitionName = \"Def01\" $galleryImageVersionName = \"0.0.1\" $sourceImageId = $imageRID $storageAccountType = \"Premium_LRS\" New-AzGalleryImageVersion -ResourceGroupName $rgNameACG -GalleryName $galleryName -GalleryImageDefinitionName $galleryImageDefinitionName -Name $galleryImageVersionName -Location $location -StorageAccountType $storageAccountType -SourceImageId $sourceImageId Once the image has been imported and the new Definition created in the Compute Gallery, you can utilizes either the Resource ID of the new Image Definition as a parameter in a template file or create new Confidential VMs from the Compute Gallery using the Azure Portal." } ]
