[
  
  {
    "title": "Export BitLocker Keys from Entra with Graph PowerShell",
    "url": "/posts/Export-Bitlocker-Keys-from-Entra/",
    "categories": "PowerShell, Azure, Microsoft Graph",
    "tags": "powershell, azure, crowdstrike, bitlocker, entra, microsoft-graph",
    "date": "2025-05-12 06:00:00 -0400",
    





    
    "snippet": "On the day of the CrowdStrike outage, countless Windows devices across the world became unstable—many of them failing to boot or respond. For some machines administrators needed one critical thing ...",
    "content": "On the day of the CrowdStrike outage, countless Windows devices across the world became unstable—many of them failing to boot or respond. For some machines administrators needed one critical thing to be able to access the disk and attempt repair: the BitLocker recovery key.If the Windows machine was registered with Intune/Entra, these recovery keys were likely stored securely in your tenant. Working with different customers and the help of PowerShell and Microsoft Graph, a script was created over that weekend that was able to retrieve BitLocker Keys quickly and at scale so that they could be provided to whatever the next step was in the recovery process.The Script: cocallaw/Entra-Get-BLKeysThe Problem  Windows machines were unable to boot due to a bad kernel driver.  To recovery, admins needed to access the disk and remove the problematic driver.  If BitLocker encryption was enabled on the OS disk, the machine would require a recovery key to unlock the drive.  Admins needed the recovery keys for numerous machines, but they were not easily accessible at scale.The Solution: PowerShell + Microsoft GraphThe following PowerShell script automates the retrieval of BitLocker recovery keys for a list of machine names. It queries Microsoft Graph to find devices by name, then pulls their associated keys.Pre-req:  The machine must be enrolled in Intune/Entra  The account connecting must have Device.Read.All and BitLockerKey.Read.All permissions in Microsoft Graph.  A CSV file with the one column named MachineName and the names of machines in the column.Running the Script  Save the script as Get-BitLockerKeys.ps1.  Open PowerShell and run the script with the path to your CSV file as the parameter -MachinesCSV.  The script will output a CSV file named DeviceRecoveryKeys.csv..\\Get-BitLockerKeys.ps1 -MachinesCSV \"C:\\path\\to\\your\\machines.csv\"Section 1: Input ValidationThe first part ensures that the file provided by the MachinesCSV parameter exists, is a .csv, and contains the expected header.Param (    [string]$MachinesCSV)if (-not (Test-Path -Path $MachinesCSV)) {    Write-Error \"The file '$MachinesCSV' does not exist. Please provide a valid file path.\"    exit}if (-not ($MachinesCSV -like \"*.csv\")) {    Write-Error \"The file '$MachinesCSV' is not a CSV file.\"    exit}$csvContent = Import-Csv -Path $MachinesCSVif (-not $csvContent[0].PSObject.Properties.Name -contains \"MachineName\") {    Write-Error \"The CSV must contain a 'MachineName' column.\"    exit}Section 2: Module Check &amp; AuthenticationThis section checks that the Microsoft Graph module is available and current, then connects with the appropriate scopes.$module = Get-Module -ListAvailable -Name Microsoft.Graphif (-not $module) {    Write-Error \"Microsoft Graph PowerShell module is not installed.\"    exit} elseif ($module.Version -lt [Version]\"2.5.0\") {    Write-Error \"Please update the Microsoft Graph module.\"    exit}try {    Connect-MgGraph -Scopes \"Device.Read.All\", \"BitLockerKey.Read.All\" -NoWelcome} catch {    Write-Error \"Failed to connect to Microsoft Graph.\"    exit}Import-Module Microsoft.Graph.Identity.DirectoryManagement -ForceSection 3: Device LookupOnce connected this part queries Microsoft Graph to find matching device records by names as listed in the CSV file provided.$vmNames = (Import-Csv -Path $MachinesCSV).MachineName$devices = @()foreach ($vmName in $vmNames) {    $devices += Get-MgDevice -Filter \"displayName eq '$vmName'\"}Section 4: Retrieve BitLocker KeysWith the list of machines stored in $devices, we pull recovery keys per device and if no recovery key info is found set NoKeyFound as the value for the Recovery Key and ID.$deviceRecoveryKeys = @()foreach ($device in $devices) {    $recoveryKeys = Get-MgInformationProtectionBitlockerRecoveryKey -Filter \"deviceId eq '$($device.DeviceId)'\"    if ($recoveryKeys) {        foreach ($key in $recoveryKeys) {            $deviceRecoveryKeys += [PSCustomObject]@{                DeviceName    = $device.DisplayName                DeviceId      = $device.DeviceId                RecoveryKeyId = $key.Id                RecoveryKey   = (Get-MgInformationProtectionBitlockerRecoveryKey -BitlockerRecoveryKeyId $key.Id -Property key).key            }        }    } else {        $deviceRecoveryKeys += [PSCustomObject]@{            DeviceName    = $device.DisplayName            DeviceId      = $device.DeviceId            RecoveryKeyId = \"NoKeyFound\"            RecoveryKey   = \"NoKeyFound\"        }    }}Section 5: Export to CSVFinally, the key information is output to a CSV file named DeviceRecoveryKeys.csv and displays the path.$outputFilePath = \"DeviceRecoveryKeys.csv\"$deviceRecoveryKeys | Export-Csv -Path $outputFilePath -NoTypeInformation$absolutePath = Resolve-Path -Path $outputFilePathWrite-Output \"Device recovery keys have been exported to '$absolutePath'.\"Section 6: Sample OutputThe output CSV DeviceRecoveryKeys.csv will follow this format:\"DeviceName\",\"DeviceId\",\"RecoveryKeyId\",\"RecoveryKey\"\"test-vm-0\",\"90e61314-d6dd-4ff8-a878-754b5b60a8be\",\"11135913-8395-458b-a7bf-dd7ef10214a2\",\"375177-019569-186362-280841-610973-240801-321277-046255\"\"test-vm-1\",\"c7165ba4-0fac-4ec5-82bf-5e4ff05dcdc8\",\"NoKeyFound\",\"NoKeyFound\"Full ScriptFor easy copy/pasting, the entire script is below in one block. You can also find it in the GitHub Repo cocallaw/Entra-Get-BLKeys.Param (    [string]$MachinesCSV)if (-not (Test-Path -Path $MachinesCSV)) {    Write-Error \"The file '$MachinesCSV' does not exist. Please provide a valid file path.\"    exit}if (-not ($MachinesCSV -like \"*.csv\")) {    Write-Error \"The file '$MachinesCSV' is not a CSV file.\"    exit}$csvContent = Import-Csv -Path $MachinesCSVif (-not $csvContent[0].PSObject.Properties.Name -contains \"MachineName\") {    Write-Error \"The CSV must contain a 'MachineName' column.\"    exit}$module = Get-Module -ListAvailable -Name Microsoft.Graphif (-not $module) {    Write-Error \"Microsoft Graph PowerShell module is not installed.\"    exit} elseif ($module.Version -lt [Version]\"2.5.0\") {    Write-Error \"Please update the Microsoft Graph module.\"    exit}try {    Connect-MgGraph -Scopes \"Device.Read.All\", \"BitLockerKey.Read.All\" -NoWelcome} catch {    Write-Error \"Failed to connect to Microsoft Graph.\"    exit}Import-Module Microsoft.Graph.Identity.DirectoryManagement -Force$vmNames = (Import-Csv -Path $MachinesCSV).MachineName$devices = @()foreach ($vmName in $vmNames) {    $devices += Get-MgDevice -Filter \"displayName eq '$vmName'\"}$deviceRecoveryKeys = @()foreach ($device in $devices) {    $recoveryKeys = Get-MgInformationProtectionBitlockerRecoveryKey -Filter \"deviceId eq '$($device.DeviceId)'\"    if ($recoveryKeys) {        foreach ($key in $recoveryKeys) {            $deviceRecoveryKeys += [PSCustomObject]@{                DeviceName    = $device.DisplayName                DeviceId      = $device.DeviceId                RecoveryKeyId = $key.Id                RecoveryKey   = (Get-MgInformationProtectionBitlockerRecoveryKey -BitlockerRecoveryKeyId $key.Id -Property key).key            }        }    } else {        $deviceRecoveryKeys += [PSCustomObject]@{            DeviceName    = $device.DisplayName            DeviceId      = $device.DeviceId            RecoveryKeyId = \"NoKeyFound\"            RecoveryKey   = \"NoKeyFound\"        }    }}$outputFilePath = \"DeviceRecoveryKeys.csv\"$deviceRecoveryKeys | Export-Csv -Path $outputFilePath -NoTypeInformation$absolutePath = Resolve-Path -Path $outputFilePathWrite-Output \"Device recovery keys have been exported to '$absolutePath'.\"Permissions ReminderThis script requires delegated or app permissions for:  Device.Read.All  BitLockerKey.Read.AllNotes  Recovery keys are only available if the device was properly enrolled and reporting compliance.  Permissions must be delegated or granted to the appropriate Graph scopes  The script exports the keys in plaintext, be cautious with the output file or update the script to output in a more secure manner  This script is designed for educational purposes and should be tested in a safe environment before use in production.Final ThoughtsThis was a good reminder that recovery isn’t just about backups—it’s also about access. PowerShell and Microsoft Graph made it possible to recover quickly during the CrowdStrike outage by tapping into the organization’s existing processes and tooling.The script is a great example of how to leverage existing tools to solve real-world problems. If you have any questions or need help with the script, feel free to reach out."
  },
  
  {
    "title": "Automating Container Builds with GitHub Actions for Upstream Changes",
    "url": "/posts/Automating-Container-Builds-for-Upstream-Changes/",
    "categories": "GitHub Actions, Docker, Automation, DevOps",
    "tags": "DevOps, Automation",
    "date": "2025-05-05 09:00:00 -0400",
    





    
    "snippet": "Keeping your container images up to date is crucial for security and performance. Whether you’re running a homelab or managing production workloads, you want to ensure that your containers are buil...",
    "content": "Keeping your container images up to date is crucial for security and performance. Whether you’re running a homelab or managing production workloads, you want to ensure that your containers are built with the latest upstream changes. However, manually keeping track of updates and rebuilding containers can be tedious.I use Tailscale to easily access services I run at home, or to easily connect to a lab environment in Azure from wherever I may be. To make this easier, I extended the Tailscale container image so I could deploy it as a subnet router and inject the address ranges that I needed advertised to my Tailnet. This made my setup more resilient, but I still had to make sure my container was using the latest version of Tailscale to make sure I was secure and prevent possible issues.Until recently, I had a simple GitHub Actions workflow that would build and push to Docker Hub, whenever I remembered to check for updates (which I didn’t do very regularly) to the upstream Tailscale image.In this post, I’ll walk through how I use a GitHub Actions workflow to automatically check for updates to an upstream Docker image, and trigger a build and push only if a new version is available. This kind of automation saves time, reduces human error, and keeps my environment reliably up to date.Why Automate Container Builds?If you’re maintaining a custom container that wraps or extends functionality from an upstream project (like Tailscale, Nginx, or Node.js), you’ll often want to:  Track upstream releases.  Rebuild your container with the new version.  Push the updated image to Docker Hub or a private registry.  Optionally commit a version change back to your repo for tracking.Doing all this manually is inefficient. Automating this with GitHub Actions ensures consistency and frees you to focus on more important tasks.The WorkflowHere’s a breakdown of the GitHub Actions workflow I use. It:  Runs every 3 days or on manual trigger.  Checks the latest stable tag of the upstream container.  Compares it against images already published.  If a newer version is available, builds and pushes multi-arch images.  Commits a version file update for visibility.Secrets You Will NeedBefore you can use this workflow, you’ll need to set up a few secrets in your GitHub repository to allow the workflow to authenticate with Docker Hub. The workflow uses a Docker Hub username and an access token for authentication.  DOCKERHUB_USERNAME: Your Docker Hub username.  DOCKERHUB_TOKEN: A Docker Hub access token with write permissions.To set these secrets, go to your GitHub repository, click on Settings, then Secrets and variables &gt; Actions. Click on New repository secret to add each secret.Workflow BreakdownLet’s walk through what each section of the workflow does and how it contributes to the build and update process.1. Triggerson:  schedule:    - cron: '0 0 */3 * *'  workflow_dispatch:  schedule: Runs the workflow automatically every 3 days.  workflow_dispatch: Allows you to manually trigger the workflow from the GitHub UI.2. Permissionspermissions:  contents: writeThis grants the workflow permission to make commits to the repository, which is necessary so that the upstream-version.txt file can be updated and pushed to the repo if there is a new upstream container version.3. Job: check-and-buildThe entire automation is contained in a single job that runs using the latest Ubuntu GitHub Actions runner.Step: Checkout Repojobs:  check-and-build:    runs-on: ubuntu-latest    steps:      - name: Checkout repo        uses: actions/checkout@v3This pulls down the repository code so that subsequent steps can read/write files like upstream-version.txt and the Dockerfile.Step: Get Latest Upstream Version      - name: Get latest Tailscale stable version        id: tailscale        run: |          VERSION=$(curl -s https://registry.hub.docker.com/v2/repositories/tailscale/tailscale/tags/?page_size=100 \\            | jq -r '.results[].name' \\            | grep -E '^v[0-9]+\\.[0-9]+(\\.[0-9]+)?$' \\            | sort -V \\            | tail -n1)          echo \"Latest stable version: $VERSION\"          echo \"version=$VERSION\" &gt;&gt; \"$GITHUB_OUTPUT\"      - name: Debug version        run: |            echo \"Detected Tailscale version: ${{ steps.tailscale.outputs.version }}\"  Uses Docker Hub’s API to fetch the latest stable tag from an upstream image.  Outputs the latest version as steps.upstream.outputs.version.Step: Check for Existing Image      - name: Check if image with version already exists        id: check        run: |          if docker manifest inspect ${{ secrets.DOCKERHUB_USERNAME }}/tailscale-sr:${{ steps.tailscale.outputs.version }} &gt; /dev/null 2&gt;&amp;1; then            echo \"Image already exists. Skipping build.\"            echo \"build_needed=false\" &gt;&gt; $GITHUB_OUTPUT          else            echo \"New version. Proceeding to build.\"            echo \"build_needed=true\" &gt;&gt; $GITHUB_OUTPUT          fi  Uses docker manifest inspect to check whether a container image for this version already exists in your Docker Hub repo.  Sets a flag build_needed to true or false.Conditional Build StepsThe following steps are only run if a new version is detected, resulting in build_needed being set to true.  Set up emulation support (QEMU) for building multi-architecture containers.  Set up Docker Buildx for advanced build features.  Log into Docker Hub using credentials stored in GitHub Secrets.  Build and push the container using the new version as a tag.      - name: Set up QEMU        if: steps.check.outputs.build_needed == 'true'        uses: docker/setup-qemu-action@v2      - name: Set up Docker Buildx        if: steps.check.outputs.build_needed == 'true'        uses: docker/setup-buildx-action@v2      - name: Login to DockerHub        if: steps.check.outputs.build_needed == 'true'        uses: docker/login-action@v2        with:          username: ${{ secrets.DOCKERHUB_USERNAME }}          password: ${{ secrets.DOCKERHUB_TOKEN }}      - name: Build and push multi-arch container        if: steps.check.outputs.build_needed == 'true'        uses: docker/build-push-action@v3        with:          context: ./docker          build-args: |            TAILSCALE_TAG=${{ steps.tailscale.outputs.version }}          platforms: linux/amd64,linux/arm64          push: true          tags: |            ${{ secrets.DOCKERHUB_USERNAME }}/tailscale-sr:latest            ${{ secrets.DOCKERHUB_USERNAME }}/tailscale-sr:${{ steps.tailscale.outputs.version }}Step: Commit Version Update      - name: Commit updated version file        if: steps.check.outputs.build_needed == 'true'        run: |          echo \"${{ steps.tailscale.outputs.version }}\" &gt; tailscale-version.txt          git config user.name \"github-actions[bot]\"          git config user.email \"github-actions[bot]@users.noreply.github.com\"          git add tailscale-version.txt          git commit -m \"Update Tailscale version to ${{ steps.tailscale.outputs.version }}\"          git pushThis writes the new version to a file, commits it, and pushes it to your GitHub repo. This gives you a version history and a way to confirm that the build occurred.The YAML Workflowname: Auto Build &amp; Publish Tailscale Containeron:  schedule:    - cron: '0 0 */3 * *'  # Every 3 days at 00:00 UTC  workflow_dispatch:permissions:  contents: write  # Needed to commit version filejobs:  check-and-build:    runs-on: ubuntu-latest    steps:      - name: Checkout repo        uses: actions/checkout@v3      - name: Get latest Tailscale stable version        id: tailscale        run: |          VERSION=$(curl -s https://registry.hub.docker.com/v2/repositories/tailscale/tailscale/tags/?page_size=100 \\            | jq -r '.results[].name' \\            | grep -E '^v[0-9]+\\.[0-9]+(\\.[0-9]+)?$' \\            | sort -V \\            | tail -n1)          echo \"Latest stable version: $VERSION\"          echo \"version=$VERSION\" &gt;&gt; \"$GITHUB_OUTPUT\"      - name: Debug version        run: |            echo \"Detected Tailscale version: ${{ steps.tailscale.outputs.version }}\"      - name: Check if image with version already exists        id: check        run: |          if docker manifest inspect ${{ secrets.DOCKERHUB_USERNAME }}/tailscale-sr:${{ steps.tailscale.outputs.version }} &gt; /dev/null 2&gt;&amp;1; then            echo \"Image already exists. Skipping build.\"            echo \"build_needed=false\" &gt;&gt; $GITHUB_OUTPUT          else            echo \"New version. Proceeding to build.\"            echo \"build_needed=true\" &gt;&gt; $GITHUB_OUTPUT          fi      - name: Set up QEMU        if: steps.check.outputs.build_needed == 'true'        uses: docker/setup-qemu-action@v2      - name: Set up Docker Buildx        if: steps.check.outputs.build_needed == 'true'        uses: docker/setup-buildx-action@v2      - name: Login to DockerHub        if: steps.check.outputs.build_needed == 'true'        uses: docker/login-action@v2        with:          username: ${{ secrets.DOCKERHUB_USERNAME }}          password: ${{ secrets.DOCKERHUB_TOKEN }}      - name: Build and push multi-arch container        if: steps.check.outputs.build_needed == 'true'        uses: docker/build-push-action@v3        with:          context: ./docker          build-args: |            TAILSCALE_TAG=${{ steps.tailscale.outputs.version }}          platforms: linux/amd64,linux/arm64          push: true          tags: |            ${{ secrets.DOCKERHUB_USERNAME }}/tailscale-sr:latest            ${{ secrets.DOCKERHUB_USERNAME }}/tailscale-sr:${{ steps.tailscale.outputs.version }}      - name: Commit updated version file        if: steps.check.outputs.build_needed == 'true'        run: |          echo \"${{ steps.tailscale.outputs.version }}\" &gt; tailscale-version.txt          git config user.name \"github-actions[bot]\"          git config user.email \"github-actions[bot]@users.noreply.github.com\"          git add tailscale-version.txt          git commit -m \"Update Tailscale version to ${{ steps.tailscale.outputs.version }}\"          git pushFinal ThoughtsAutomating container builds with GitHub Actions is a powerful way to keep your images up to date without manual intervention. This workflow can be adapted for any upstream project, and you can customize it further based on your needs.  Consider adding notifications (e.g., Slack, email) to alert you when a new version is built.  You can also extend the workflow to run tests against the new image before pushing it.  Explore other GitHub Actions to enhance your CI/CD pipeline.If you’ve built similar automation’s or have tips for improving this setup, let me know—I’d love to hear how others are tackling this."
  },
  
  {
    "title": "Create Confidential Compute Capable Custom Images from Windows CVMs",
    "url": "/posts/Creating-Confidentail-Compute-Images-from-CVMs/",
    "categories": "Confidential Compute",
    "tags": "images",
    "date": "2025-01-06 12:00:00 -0500",
    





    
    "snippet": "Confidential Compute Custom ImagesCreating a custom image from an Azure Confidential Compute VM (CVM) requires following a different process than you would for a regular Azure VM. This is due to th...",
    "content": "Confidential Compute Custom ImagesCreating a custom image from an Azure Confidential Compute VM (CVM) requires following a different process than you would for a regular Azure VM. This is due to the design of CVMs, which utilize an OS disk and a small encrypted data disk that contains the VM Guest State(VMGS) information. As a result, using the Capture button in the Azure portal or the New-AzImage command in Azure PowerShell will not produce the desired results.This process to create a custom image from a CVM is necessary so that the captured image is free of and VM Guest State information and the correct properties are set for the image version and reference in a Azure Compute Gallery.The steps outlined below require that you have access to the Azure Subscriptions containing the resources, and the current version of both Azure CLI and AzCopy installed. Commands were tested using Azure CLI from a Powershell Core session.  This process to create a custom image is for an existing Windows based CVM using with Confidential OS disk encryption enabled using either PMK or CMK.Prepare the CVM OS for CaptureOnce the customization of the Windows OS is complete, the next step is to Disable BitLocker, wait for the decryption to complete, and then run Sysprep.To disable BitLocker and check the decryption status of the OS disk, you can use the following commands in an elevated Command Prompt.# Disable BitLockermanage-bde -off C:# Check the decryption statusmanage-bde -status C:When the decryption status returns as Fully Decrypted, Sysprep can now be run. Selecting Generalize and Shutdown as the options.Creating the Custom ImageCollect OS Disk InformationThe first step is to make sure the CVM the image is being created from is fully deallocated and then collect information about the OS disk. To do so we will need to know the resource group name, VM name, and region that the CVM is located in, we will set these as variables for easy reference.$region = \"North Europe\"$resourceGroupName = \"rg-custimg-lab-01\"$vmName = \"custcvm-01\"With the variables set, verify the VM is deallocated# Deallocate the VMaz vm deallocate --name $vmname --resource-group $resourceGroupName# Collect the OS Disk information$disk_name = (az vm show --name $vmname --resource-group $resourceGroupName | jq -r .storageProfile.osDisk.name)$disk_url = (az disk grant-access --duration-in-seconds 3600 --name $disk_name --resource-group $resourceGroupName | jq -r .accessSas)Create a Storage Account for the VHDNext, create a storage account, this will be used to store the exported VHD of the CVMs OS disk before it is uploaded to the Compute Gallery. For this part of the process, you will need to know the name of the Storage Account and Container that will be created.$storageAccountName = \"stgcvmvhd01\"$storageContainerName = \"cvmimages\"$referenceVHD = \"${vmName}.vhd\"Create the Storage Account and Container# Create Storage Accountaz storage account create --resource-group ${resourceGroupName} --name ${storageAccountName} --location $region --sku \"Standard_LRS\"# Create a container in the Storage Accountaz storage container create --name $storageContainerName --account-name $storageAccountName --resource-group $resourceGroupNameWith the Storage Account and Container created, generate a Shared Access Signature (SAS) token to upload the disk image to the container. Be sure to set the expiry date to a date in the future.# Generate a SAS token for the container$container_sas=(az storage container generate-sas --name $storageContainerName --account-name $storageAccountName --auth-mode key --expiry 2025-01-01 --https-only --permissions dlrw -o tsv)Using the SAS token and information collected, the VHD can now be exported to the Storage Account using AzCopy.# Build the Blob URL$blob_url=\"https://${storageAccountName}.blob.core.windows.net/$storageContainerName/$referenceVHD\"# Export the VHD using AzCopyazcopy copy \"$disk_url\" \"${blob_url}?${container_sas}\"Upload the Image to a Compute GalleryWith the VHD successfully exported to the Storage Account, the next step is to create an image definition in the Compute Gallery and upload the VHD to the gallery as a new version. For this part of the process, you will need to know the name of the Compute Gallery, Image Definition, Offer, Publisher, SKU, and Version number that will be created.$galleryName = \"acglabneu01\"$imageDefinitionName = \"cvmimage01\"$OfferName = \"offername01\"$PublisherName = \"pubname01\"$SkuName = \"skuname01\"$galleryImageVersion = \"1.0.0\"If a Compute Gallery does not already exist, create one using the following command, and create an Image Definition that has the required features and parameters set for Confidential VM support.# Create the Compute Galleryaz sig create --resource-group $resourceGroupName --gallery-name $galleryName# Create the Image Definitionaz sig image-definition create --resource-group  $resourceGroupName --location $region --gallery-name $galleryName --gallery-image-definition $imageDefinitionName --publisher $PublisherName --offer $OfferName --sku $SkuName --os-type windows --os-state Generalized --hyper-v-generation V2  --features SecurityType=ConfidentialVMSupportedTo upload the VHD to the Compute Gallery, the ID of the Storage Account that contains tehVHD is required when creating the image version. This can be obtained using the following command.# Get the Storage Account ID$storageAccountId=(az storage account show --name $storageAccountName --resource-group $resourceGroupName | jq -r .id)With everything in place, the final step is to create the image version in the Compute Gallery using the VHD that was exported from the CVM.# Create the Image Versionaz sig image-version create --resource-group $resourceGroupName --gallery-name $galleryName --gallery-image-definition $imageDefinitionName --gallery-image-version $galleryImageVersion --os-vhd-storage-account $storageAccountId --os-vhd-uri $blob_urlThe Full Image Export ProcessBelow is the full process to export the OS disk from the CVM, create the image, and upload it to the Compute Gallery.# Set Variables$region = \"North Europe\"$resourceGroupName = \"rg-custimg-lab-01\"$vmName = \"custcvm-01\"$storageAccountName = \"stgcvmvhd01\"$storageContainerName = \"cvmimages\"$referenceVHD = \"${vmName}.vhd\"$galleryName = \"acglabneu01\"$imageDefinitionName = \"cvmimage01\"$OfferName = \"offername01\"$PublisherName = \"pubname01\"$SkuName = \"skuname01\"$galleryImageVersion = \"1.0.0\"# Deallocate the VMaz vm deallocate --name $vmName --resource-group $resourceGroupName# Collect the OS Disk information$disk_name = (az vm show --name $vmName --resource-group $resourceGroupName | jq -r .storageProfile.osDisk.name)$disk_url = (az disk grant-access --duration-in-seconds 3600 --name $disk_name --resource-group $resourceGroupName | jq -r .accessSas)# Create Storage Accountaz storage account create --resource-group ${resourceGroupName} --name ${storageAccountName} --location $region --sku \"Standard_LRS\"# Create a container in the Storage Accountaz storage container create --name $storageContainerName --account-name $storageAccountName --resource-group $resourceGroupName# Generate a SAS token for the container$container_sas=(az storage container generate-sas --name $storageContainerName --account-name $storageAccountName --auth-mode key --expiry 2025-01-01 --https-only --permissions dlrw -o tsv)# Build the Blob URL$blob_url=\"https://${storageAccountName}.blob.core.windows.net/$storageContainerName/$referenceVHD\"# Export the VHD using AzCopyazcopy copy \"$disk_url\" \"${blob_url}?${container_sas}\"# Create the Compute Galleryaz sig create --resource-group $resourceGroupName --gallery-name $galleryName# Create the Image Definitionaz sig image-definition create --resource-group  $resourceGroupName --location $region --gallery-name $galleryName --gallery-image-definition $imageDefinitionName --publisher $PublisherName --offer $OfferName --sku $SkuName --os-type windows --os-state Generalized --hyper-v-generation V2  --features SecurityType=ConfidentialVMSupported# Get the Storage Account ID$storageAccountId=(az storage account show --name $storageAccountName --resource-group $resourceGroupName | jq -r .id)# Create the Image Versionaz sig image-version create --resource-group $resourceGroupName --gallery-name $galleryName --gallery-image-definition $imageDefinitionName --gallery-image-version $galleryImageVersion --os-vhd-storage-account $storageAccountId --os-vhd-uri $blob_url"
  },
  
  {
    "title": "Creating Confidential Compute Capable Custom Images from Standard Windows VMs",
    "url": "/posts/Creating-Confidential-Compute-Capable-Custom-Images/",
    "categories": "Confidential Compute",
    "tags": "images",
    "date": "2025-01-05 12:00:00 -0500",
    





    
    "snippet": "Confidential Compute and Custom ImagesCreating a custom image that can be used to create Azure Confidential Compute (ACC) VMs is similar to creating a standard custom image, but with a slight twist...",
    "content": "Confidential Compute and Custom ImagesCreating a custom image that can be used to create Azure Confidential Compute (ACC) VMs is similar to creating a standard custom image, but with a slight twist when it comes to how the image is captured. This post covers how to create a custom image that could be used to provision a new ACC VM using either Customer Managed Key (CMK) or Platform Managed Key (PMK) encryption in any Azure region that has ACC capable AMD VM SKUs.It is important to use the proper settings when creating the VM used to generate the custom image and capture the custom image, so that settings and features such as BitLocker are not enabled, which would require additional steps before capturing and possibly cause issues when using the captured image to provision a new CVM.The steps outlined below require that you have access to the Azure Subscriptions containing the resources, and the current version of Azure PowerShell installed. Commands were tested using Azure PowerShell from a Powershell Core session.Creating the VM for Custom Image Capture      From the Azure Portal, Select Virtual Machine and Create New VM        On the Instance Detail page set Security Type to Standard, and select a non-ACC VM SKU            On the Disks tab, set Key Management to Platform-managed Key and leave Encryption at host unchecked            Once the Custom Image VM has been deployed, connect to the machine and perform any customization tasks required.        When all customizations are complete, run Sysprep with OOBE, Generalize and Shutdown selected            Once Sysprep has completed, the VM is ready to be captured  Capture the Custom ImageFrom an Azure PowerShell session connected to the Subscription that contains the Custom Image VM, set the proper values for the variables $vmName, $rgName, $location and $imageName.$vmName = \"customvm01\"$rgNameCustImg = \"rg-custom-img-01\"$location = \"North Europe\"$imageName = \"image-01\"With the variables set run the following commands to create a Manged Image resource from the OS disk of the Custom Image VM.# Verify that the VM is deallocatedStop-AzVM -ResourceGroupName $rgNameCustImg -Name $vmName -Force# Set the status of the VM to generalizedSet-AzVm -ResourceGroupName $rgNameCustImg -Name $vmName -Generalized# Store the VM details in a variable$vm = Get-AzVM -Name $vmName -ResourceGroupName $rgNameCustImg# Create the image configuration $imageConfig = New-AzImageConfig -Location $location -SourceVirtualMachineId $vm.Id -HyperVGeneration V2# Create an image from the VMNew-AzImage -ImageName $imageName -ResourceGroupName $rgNameCustImg -Image $imageConfigImporting the Custom Image into Azure Compute GalleryAzure PortalIn the Azure Compute Gallery create a new Image Definition, with the Security Type set to Trusted launch and confidential VM supportedOn the next page select the Managed Image Resource to import to the Compute Gallery for the Image DefinitionAzure PowerShellTo import the Managed Image into the Compute Gallery, there first needs to be a new Image Definition created and then the Managed Image imported as a new version of the image definition.To create the new Image Definition, set the following variables$rgNameACG  = \"rg-compute-gallery-01\"$location = \"North Europe\"$galleryName = \"acgallery01\"$galleryImageDefinitionName = \"Def01\"$publisherName = \"Publisher01\"$offerName = \"Offer01\"$skuName = \"Win11-24H2\"$description = \"Windows 11 24H2\"# Variables to set the features of the Image Definition$ConfidentialVMSupported = @{Name='SecurityType';Value='TrustedLaunchAndConfidentialVmSupported'}$IsHibernateSupported = @{Name='IsHibernateSupported';Value='False'}$features = @($ConfidentialVMSupported,$IsHibernateSupported)With the variables set, run the following command to create the new Image Definition in the Compute GalleryNew-AzGalleryImageDefinition -ResourceGroupName $rgNameACG -GalleryName $galleryName -Name $galleryImageDefinitionName -Location $location -Publisher $publisherName -Offer $offerName -Sku $skuName -OsState \"Generalized\" -OsType \"Windows\" -Description $description -Feature $features -HyperVGeneration \"V2\"Now that there is an Image Definition in the Compute Gallery, the Managed Image that was created from the Custom Image VM can be imported as a version of the Image Definition. To do this, first the Resource ID of the Managed Image needs to be retrieved.$imageRID = (Get-AzImage -ResourceGroupName $rgNameCustImg -ImageName $imageName).Id$rgNameACG = \"rg-compute-gallery-01\"$location = \"North Europe\"$galleryName = \"acgallery01\"$galleryImageDefinitionName = \"Def01\"$galleryImageVersionName = \"0.0.1\"$sourceImageId = $imageRID$storageAccountType = \"Premium_LRS\"New-AzGalleryImageVersion -ResourceGroupName $rgNameACG -GalleryName $galleryName -GalleryImageDefinitionName $galleryImageDefinitionName -Name $galleryImageVersionName -Location $location -StorageAccountType $storageAccountType -SourceImageId $sourceImageIdOnce the image has been imported and the new Definition created in the Compute Gallery, you can utilizes either the Resource ID of the new Image Definition as a parameter in a template file or create new Confidential VMs from the Compute Gallery using the Azure Portal."
  }
  
]

